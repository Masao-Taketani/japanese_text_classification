{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I referred to the following webpages for the implementation.\n",
    "- Implementation of Transformer<br>\n",
    "https://qiita.com/halhorn/items/c91497522be27bde17ce<br>\n",
    "https://github.com/kpot/keras-transformer/tree/master/keras_transformer<br>\n",
    "https://github.com/Lsdefine/attention-is-all-you-need-keras<br>\n",
    "- Usage of \"\\_\\_call\\_\\_\" method<br>\n",
    "https://qiita.com/kyo-bad/items/439d8cc3a0424c45214a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Activation, Layer, Embedding, Input, Reshape, Lambda, Add\n",
    "from keras import backend as K\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.utils import plot_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 8000\n",
    "d_model = 512\n",
    "MAX_LEN = 716\n",
    "class_num = 9\n",
    "PAD_ID = 0\n",
    "warmup_steps = 4000\n",
    "NUM_TRAIN = 5893\n",
    "NUM_TEST = 1474\n",
    "batch_size = 16\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(example):\n",
    "    features = tf.parse_single_example(\n",
    "        example,\n",
    "        features={\n",
    "            \"X\": tf.FixedLenFeature([MAX_LEN], dtype=tf.float32),\n",
    "            \"Y\": tf.FixedLenFeature((class_num,), dtype=tf.float32)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    X = features[\"X\"]\n",
    "    Y = features[\"Y\"]\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterator(tfrecord_path, data_len):\n",
    "    dataset = tf.data.TFRecordDataset([tfrecord_path]).map(parse)\n",
    "    dataset = dataset.repeat(-1).batch(data_len)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    X, Y = iterator.get_next()\n",
    "    X = tf.reshape(X, [-1, MAX_LEN])\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.1 s, sys: 1.17 s, total: 3.27 s\n",
      "Wall time: 2.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "x_train, y_train = iterator(\"train_transformer_with_sp.tfrecord\", NUM_TRAIN)\n",
    "x_test, y_test = iterator(\"test_transformer_with_sp.tfrecord\", NUM_TEST)\n",
    "\n",
    "x_train = tf.Session().run(x_train)\n",
    "y_train = tf.Session().run(y_train)\n",
    "x_test = tf.Session().run(x_test)\n",
    "y_test = tf.Session().run(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5893, 716), (5893, 9), (1474, 716), (1474, 9))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention():\n",
    "    ## hidden_dim has to be multiples of head_num\n",
    "    def __init__(self, max_len, hidden_dim=512, head_num=8, dropout_rate=0.1, *args, **kwargs):\n",
    "        self.max_len = max_len\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.head_num = head_num\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.q_dense_layer = Dense(hidden_dim, use_bias=False)\n",
    "        self.k_dense_layer = Dense(hidden_dim, use_bias=False)\n",
    "        self.v_dense_layer = Dense(hidden_dim, use_bias=False)\n",
    "        self.output_dense_layer = Dense(hidden_dim, use_bias=False)\n",
    "        self.attention_dropout_layer = Dropout(dropout_rate)\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        def reshape(x):\n",
    "            x = tf.reshape(x, [-1, self.max_len, self.head_num, self.hidden_dim // self.head_num])\n",
    "            return tf.transpose(x, [0, 2, 1, 3])\n",
    "        \n",
    "        out = Lambda(reshape)(x)\n",
    "        return out\n",
    "    \n",
    "    def combine_heads(self, heads):\n",
    "        def reshape(x):\n",
    "            heads = tf.transpose(x, [0, 2, 1, 3])\n",
    "            return tf.reshape(x, [-1, self.max_len, self.hidden_dim])\n",
    "        \n",
    "        out = Lambda(reshape)(heads)\n",
    "        return out\n",
    "        \n",
    "    def __call__(self, query, memory):\n",
    "        #two arguments of query and memory are already encoded as embedded vectors for all words\n",
    "        q = self.q_dense_layer(query)\n",
    "        k = self.k_dense_layer(memory)\n",
    "        v = self.v_dense_layer(memory)\n",
    "        \n",
    "        q = self.split_heads(q)\n",
    "        k = self.split_heads(k)\n",
    "        v = self.split_heads(v)\n",
    "        \n",
    "        #for scaled dot-product\n",
    "        depth_inside_each_head = self.hidden_dim // self.head_num\n",
    "        q = Lambda(lambda x: x * (depth_inside_each_head ** -0.5))(q)\n",
    "        \n",
    "        #q.shape = (batch_size, head_num, query_len, emb_dim)\n",
    "        #k.shape = (batch_size, head_num, memory_len, emb_dim)\n",
    "        #batch_dot(q, k).shape = (batch_size, head_num, query_len, memory_len)\n",
    "        score = Lambda(lambda x: K.batch_dot(x[0], x[1], axes=[3, 3]))([q, k])\n",
    "        normalized_score = Activation(\"softmax\")(score)\n",
    "        normalized_score = self.attention_dropout_layer(normalized_score)\n",
    "        \n",
    "        #normalized_score.shape = (batch_size, head_num, query_length, memory_length)\n",
    "        #v.shape = (batch_size, head_num, memory_length, depth)\n",
    "        #attention_weighted_output.shape = (batch_size, head_num, query_length, depth)\n",
    "        attention_weighted_output = Lambda(lambda x: K.batch_dot(x[0], x[1], axes=[3, 2]))([normalized_score, v])\n",
    "        attention_weighted_output = self.combine_heads(attention_weighted_output)\n",
    "        return self.output_dense_layer(attention_weighted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SlefAttention class inherits MultiheadAttention class so that it can make query and memory come from the same source.\n",
    "class SelfAttention(MultiheadAttention):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def __call__(self, query):\n",
    "        return super().__call__(query, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForwardNetwork():\n",
    "    \n",
    "    def __init__(self, hidden_dim, dropout_rate, *args, **kwargs):\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.first_dense_layer = Dense(hidden_dim*4, use_bias=True, activation=\"relu\")\n",
    "        self.second_dense_layer = Dense(hidden_dim, use_bias=True, activation=\"linear\")\n",
    "        self.dropout_layer = Dropout(dropout_rate)\n",
    "        \n",
    "    def __call__(self, inputs):\n",
    "        # make the network more flexible to learn for the first dense layer(non-linear transformation is used),\n",
    "        # and put the network back into the same hidden dim as original(linear transformation is used)\n",
    "        x = self.first_dense_layer(inputs)\n",
    "        x = self.dropout_layer(x)\n",
    "        return self.second_dense_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(Layer):\n",
    "    def __init__(self, axis=-1, **kwargs):\n",
    "        self.axis = axis\n",
    "        super(LayerNormalization, self).__init__(**kwargs)\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config[\"axis\"] = self.axis\n",
    "        return config\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        hidden_dim = input_shape[-1]\n",
    "        self.scale = self.add_weight(\"layer_norm_scale\", shape=[hidden_dim],\n",
    "                                    initializer=\"ones\")\n",
    "        self.shift = self.add_weight(\"layer_norm_shift\", shape=[hidden_dim],\n",
    "                                    initializer=\"zeros\")\n",
    "        super(LayerNormalization, self).build(input_shape)\n",
    "        \n",
    "    def call(self, inputs, epsilon=1e-6):\n",
    "        mean = K.mean(inputs, axis=[-1], keepdims=True)\n",
    "        variance = K.var(inputs, axis=[-1], keepdims=True)\n",
    "        normalized_inputs = (inputs - mean) / (K.sqrt(variance) + epsilon)\n",
    "        return normalized_inputs * self.scale + self.shift\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreLayerNormPostResidualConnectionWrapper():\n",
    "    def __init__(self, layer, dropout_rate, *args, **kwargs):\n",
    "        self.layer = layer\n",
    "        self.layer_norm = LayerNormalization()\n",
    "        self.dropout_layer = Dropout(dropout_rate)\n",
    "        \n",
    "    def __call__(self, inputs, *args, **kwargs):\n",
    "        x = self.layer_norm(inputs)\n",
    "        x = self.layer(x)\n",
    "        outputs = self.dropout_layer(x)\n",
    "        results = Add()([inputs, outputs])\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddPositionalEncoding(Layer): \n",
    "    def call(self, inputs):\n",
    "        data_type = inputs.dtype\n",
    "        batch_size, max_len, emb_dim = tf.unstack(tf.shape(inputs))\n",
    "        # i is from 0 to 255 when emb_dim is 512\n",
    "        #so the doubled_i is from 0 to 510\n",
    "        doubled_i = K.arange(emb_dim) // 2 * 2\n",
    "        exponent = K.tile(K.expand_dims(doubled_i, 0), [max_len, 1])\n",
    "        denominator_matrix = K.pow(10000.0, K.cast(exponent / emb_dim, data_type))\n",
    "        \n",
    "        # since cos(x) = sin(x + Ï€/2), we convert the series of [sin, cos, sin, cos, ...]\n",
    "        # into [sin, sin, sin, sin, ...]\n",
    "        to_convert = K.cast(K.arange(emb_dim) % 2, data_type) * math.pi / 2\n",
    "        convert_matrix = K.tile(tf.expand_dims(to_convert, 0), [max_len, 1])\n",
    "        \n",
    "        seq_pos = K.arange(max_len)\n",
    "        numerator_matrix = K.cast(K.tile(K.expand_dims(seq_pos, 1), [1, emb_dim]), data_type)\n",
    "        \n",
    "        positinal_encoding = K.sin(numerator_matrix / denominator_matrix + convert_matrix)\n",
    "        batched_positional_encoding = K.tile(K.expand_dims(positinal_encoding, 0), [batch_size, 1, 1])\n",
    "        return inputs + batched_positional_encoding\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeZeroPads(Layer):\n",
    "    def __init__(self, seq_len, vocab_size, emb_dim, data_type=\"float32\", *args, **kwargs):\n",
    "        self.emb_dim = emb_dim\n",
    "        super(MakeZeroPads, self).__init__(*args, **kwargs)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        mask_for_pads = tf.to_float(tf.not_equal(inputs, PAD_ID))\n",
    "        pads_masked_embedding = inputs * mask_for_pads\n",
    "        return pads_masked_embedding * (self.emb_dim ** 0.5)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder():\n",
    "    def __init__(self, vocab_size, max_len, stack_num, head_num, emb_dim, dropout_rate, *args, **kwargs):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_len = max_len\n",
    "        self.stack_num = stack_num\n",
    "        self.head_num = head_num\n",
    "        self.emb_dim = emb_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.embedding_layer = Embedding(self.vocab_size,\n",
    "                           self.emb_dim,\n",
    "                           embeddings_initializer=RandomNormal(mean=0.0, stddev=self.emb_dim**-0.5)\n",
    "                          )\n",
    "        self.make_zero_pads_layer = MakeZeroPads(self.max_len, vocab_size, emb_dim)\n",
    "        self.add_pos_enc_layer = AddPositionalEncoding()\n",
    "        self.input_dropout_layer = Dropout(dropout_rate)\n",
    "        \n",
    "        self.attention_block_list = []\n",
    "        for _ in range(stack_num):\n",
    "            self_attention_layer = SelfAttention(self.max_len, self.emb_dim, self.head_num, self.dropout_rate)\n",
    "            pffn_layer = PositionwiseFeedForwardNetwork(self.emb_dim, self.dropout_rate)\n",
    "            self.attention_block_list.append([\n",
    "                PreLayerNormPostResidualConnectionWrapper(self_attention_layer, dropout_rate),\n",
    "                PreLayerNormPostResidualConnectionWrapper(pffn_layer, dropout_rate)\n",
    "            ])\n",
    "        self.output_layer_norm = LayerNormalization()\n",
    "        \n",
    "    def __call__(self, inputs):\n",
    "        x = self.embedding_layer(inputs)\n",
    "        x = self.make_zero_pads_layer(x)\n",
    "        x = self.add_pos_enc_layer(x)\n",
    "        x = self.input_dropout_layer(x)\n",
    "        \n",
    "        for i, set_of_layers_list in enumerate(self.attention_block_list):\n",
    "            self_attention_layer, pffn_layer = tuple(set_of_layers_list)\n",
    "            x = self_attention_layer(x)\n",
    "            x = pffn_layer(x)\n",
    "            \n",
    "        return self.output_layer_norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 716)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 716, 512)     4096000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "make_zero_pads_1 (MakeZeroPads) (None, 716, 512)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_positional_encoding_1 (AddP (None, 716, 512)     0           make_zero_pads_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 716, 512)     0           add_positional_encoding_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 716, 512)     1024        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 716, 512)     262144      layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 8, 716, 64)   0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 716, 512)     262144      layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 8, 716, 64)   0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 8, 716, 64)   0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 8, 716, 716)  0           lambda_4[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 8, 716, 716)  0           lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 716, 512)     262144      layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 8, 716, 716)  0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 8, 716, 64)   0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 8, 716, 64)   0           dropout_2[0][0]                  \n",
      "                                                                 lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 716, 512)     0           lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 716, 512)     262144      lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 716, 512)     0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 716, 512)     0           dropout_1[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_2 (LayerNor (None, 716, 512)     1024        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 716, 2048)    1050624     layer_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 716, 2048)    0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 716, 512)     1049088     dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 716, 512)     0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 716, 512)     0           add_1[0][0]                      \n",
      "                                                                 dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 716, 512)     1024        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 716, 512)     262144      layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 8, 716, 64)   0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 716, 512)     262144      layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 8, 716, 64)   0           lambda_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 8, 716, 64)   0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 8, 716, 716)  0           lambda_11[0][0]                  \n",
      "                                                                 lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 8, 716, 716)  0           lambda_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 716, 512)     262144      layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 8, 716, 716)  0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 8, 716, 64)   0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 8, 716, 64)   0           dropout_6[0][0]                  \n",
      "                                                                 lambda_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_14 (Lambda)              (None, 716, 512)     0           lambda_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 716, 512)     262144      lambda_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 716, 512)     0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 716, 512)     0           add_2[0][0]                      \n",
      "                                                                 dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_4 (LayerNor (None, 716, 512)     1024        add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 716, 2048)    1050624     layer_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 716, 2048)    0           dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 716, 512)     1049088     dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 716, 512)     0           dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 716, 512)     0           add_3[0][0]                      \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_5 (LayerNor (None, 716, 512)     1024        add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 716, 512)     262144      layer_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              (None, 8, 716, 64)   0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 716, 512)     262144      layer_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_18 (Lambda)              (None, 8, 716, 64)   0           lambda_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 8, 716, 64)   0           dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_19 (Lambda)              (None, 8, 716, 716)  0           lambda_18[0][0]                  \n",
      "                                                                 lambda_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 8, 716, 716)  0           lambda_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 716, 512)     262144      layer_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 8, 716, 716)  0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_17 (Lambda)              (None, 8, 716, 64)   0           dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_20 (Lambda)              (None, 8, 716, 64)   0           dropout_10[0][0]                 \n",
      "                                                                 lambda_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_21 (Lambda)              (None, 716, 512)     0           lambda_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 716, 512)     262144      lambda_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 716, 512)     0           dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 716, 512)     0           add_4[0][0]                      \n",
      "                                                                 dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_6 (LayerNor (None, 716, 512)     1024        add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 716, 2048)    1050624     layer_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 716, 2048)    0           dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 716, 512)     1049088     dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 716, 512)     0           dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 716, 512)     0           add_5[0][0]                      \n",
      "                                                                 dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_7 (LayerNor (None, 716, 512)     1024        add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 716, 512)     262144      layer_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_22 (Lambda)              (None, 8, 716, 64)   0           dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 716, 512)     262144      layer_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_25 (Lambda)              (None, 8, 716, 64)   0           lambda_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_23 (Lambda)              (None, 8, 716, 64)   0           dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_26 (Lambda)              (None, 8, 716, 716)  0           lambda_25[0][0]                  \n",
      "                                                                 lambda_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 8, 716, 716)  0           lambda_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 716, 512)     262144      layer_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 8, 716, 716)  0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_24 (Lambda)              (None, 8, 716, 64)   0           dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_27 (Lambda)              (None, 8, 716, 64)   0           dropout_14[0][0]                 \n",
      "                                                                 lambda_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_28 (Lambda)              (None, 716, 512)     0           lambda_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 716, 512)     262144      lambda_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 716, 512)     0           dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 716, 512)     0           add_6[0][0]                      \n",
      "                                                                 dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_8 (LayerNor (None, 716, 512)     1024        add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 716, 2048)    1050624     layer_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 716, 2048)    0           dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 716, 512)     1049088     dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 716, 512)     0           dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 716, 512)     0           add_7[0][0]                      \n",
      "                                                                 dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_9 (LayerNor (None, 716, 512)     1024        add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 716, 512)     262144      layer_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_29 (Lambda)              (None, 8, 716, 64)   0           dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 716, 512)     262144      layer_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_32 (Lambda)              (None, 8, 716, 64)   0           lambda_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_30 (Lambda)              (None, 8, 716, 64)   0           dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_33 (Lambda)              (None, 8, 716, 716)  0           lambda_32[0][0]                  \n",
      "                                                                 lambda_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 8, 716, 716)  0           lambda_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 716, 512)     262144      layer_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 8, 716, 716)  0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_31 (Lambda)              (None, 8, 716, 64)   0           dense_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_34 (Lambda)              (None, 8, 716, 64)   0           dropout_18[0][0]                 \n",
      "                                                                 lambda_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_35 (Lambda)              (None, 716, 512)     0           lambda_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 716, 512)     262144      lambda_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 716, 512)     0           dense_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 716, 512)     0           add_8[0][0]                      \n",
      "                                                                 dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_10 (LayerNo (None, 716, 512)     1024        add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 716, 2048)    1050624     layer_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 716, 2048)    0           dense_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 716, 512)     1049088     dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 716, 512)     0           dense_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 716, 512)     0           add_9[0][0]                      \n",
      "                                                                 dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_11 (LayerNo (None, 716, 512)     1024        add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 716, 512)     262144      layer_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_36 (Lambda)              (None, 8, 716, 64)   0           dense_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 716, 512)     262144      layer_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_39 (Lambda)              (None, 8, 716, 64)   0           lambda_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_37 (Lambda)              (None, 8, 716, 64)   0           dense_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_40 (Lambda)              (None, 8, 716, 716)  0           lambda_39[0][0]                  \n",
      "                                                                 lambda_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 8, 716, 716)  0           lambda_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 716, 512)     262144      layer_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 8, 716, 716)  0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_38 (Lambda)              (None, 8, 716, 64)   0           dense_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_41 (Lambda)              (None, 8, 716, 64)   0           dropout_22[0][0]                 \n",
      "                                                                 lambda_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_42 (Lambda)              (None, 716, 512)     0           lambda_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 716, 512)     262144      lambda_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 716, 512)     0           dense_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 716, 512)     0           add_10[0][0]                     \n",
      "                                                                 dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_12 (LayerNo (None, 716, 512)     1024        add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 716, 2048)    1050624     layer_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 716, 2048)    0           dense_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, 716, 512)     1049088     dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 716, 512)     0           dense_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 716, 512)     0           add_11[0][0]                     \n",
      "                                                                 dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_13 (LayerNo (None, 716, 512)     1024        add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_43 (Lambda)              (None, 512)          0           layer_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, 9)            4617        lambda_43[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 23,003,657\n",
      "Trainable params: 23,003,657\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Transformer classification model\n",
    "inputs = Input((MAX_LEN,))\n",
    "transformer_encoder = Encoder(vocab_size=vocab_size, stack_num=6, head_num=8, emb_dim=512, dropout_rate=0.1, max_len=MAX_LEN)\n",
    "encoder_output = transformer_encoder(inputs)\n",
    "#Since the task is text classification, we just need the first token for each sample\n",
    "summarized_vecs = Lambda(lambda x: x[:, 0, :])(encoder_output)\n",
    "outputs = Dense(class_num)(summarized_vecs)\n",
    "model = Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file=\"transformer_encoder.png\", show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning rate for Adam\n",
    "class LRSchedulerPerStep(Callback):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        self.d_model = d_model\n",
    "        self.warmup_step = warmup_steps\n",
    "        self.step_num = 0\n",
    "        \n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        self.step_num += 1\n",
    "        updated_lr = self.d_model * min(self.step_num ** (-0.5), self.step_num * (self.warmup_step ** (-1.5)))\n",
    "        K.set_value(self.model.optimizer.lr, updated_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = LRSchedulerPerStep(d_model, warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About customized loss function\n",
    "https://stackoverflow.com/questions/50063613/add-loss-function-in-keras<br>\n",
    "https://github.com/kpot/keras-transformer/blob/b9d4e76c535c0c62cadc73e37416e4dc18b635ca/keras_transformer/bert.py#L212<br>\n",
    "https://github.com/tensorflow/models/blob/b9ef963d1e84da0bb9c0a6039457c39a699ea149/official/transformer/v2/metrics.py#L47<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smoothed loss function\n",
    "class SmoothedLossSparseCategoricalXEntropy:\n",
    "    def __init__(self, smoothing, class_num):\n",
    "        self.smoothing = smoothing\n",
    "        self.class_num = class_num\n",
    "        \n",
    "    def __call__(self, y_true, y_pred):\n",
    "        confidence = 1.0 - self.smoothing\n",
    "        low_confidence = (1.0 - confidence) / tf.cast(self.class_num - 1, tf.float32)\n",
    "        smoothed_labels = tf.one_hot(\n",
    "            tf.cast(y_true, tf.int32),\n",
    "            depth=self.class_num,\n",
    "            on_value=confidence,\n",
    "            off_value=low_confidence\n",
    "        )\n",
    "        xentropy = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            logits=y_pred,\n",
    "            labels=smoothed_labels\n",
    "        )\n",
    "        \n",
    "        lowest_loss = -(confidence * tf.log(confidence) + \n",
    "                       tf.cast(self.class_num -1, tf.float32) * low_confidence * tf.log(low_confidence + 1e-20))\n",
    "        final_loss = xentropy - lowest_loss\n",
    "        return final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5893,), (1474,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.argmax(y_train, axis=-1)\n",
    "y_test = np.argmax(y_test, axis=-1)\n",
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=opt, loss=SmoothedLossSparseCategoricalXEntropy(smoothing=0.1, class_num=class_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5893 samples, validate on 1474 samples\n",
      "Epoch 1/30\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 974.9758 - val_loss: 2331.3364\n",
      "Epoch 2/30\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 965.6583 - val_loss: 844.5497\n",
      "Epoch 3/30\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 919.7374 - val_loss: 2032.6272\n",
      "Epoch 4/30\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 902.4449 - val_loss: 618.2776\n",
      "Epoch 5/30\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 829.1413 - val_loss: 804.5805\n",
      "Epoch 6/30\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 842.8073 - val_loss: 1510.1228\n",
      "Epoch 7/30\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 884.1554 - val_loss: 1658.1582\n",
      "Epoch 8/30\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 777.7955 - val_loss: 935.3201\n",
      "Epoch 9/30\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 843.0859 - val_loss: 1654.7361\n",
      "Epoch 10/30\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 737.7503 - val_loss: 719.4438\n",
      "Epoch 11/30\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 814.9801 - val_loss: 459.2993\n",
      "Epoch 12/30\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 747.3028 - val_loss: 1123.8474\n",
      "Epoch 13/30\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 780.5081 - val_loss: 681.2123\n",
      "Epoch 14/30\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 712.2388 - val_loss: 403.6831\n",
      "Epoch 15/30\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 718.8174 - val_loss: 808.1876\n",
      "Epoch 16/30\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 674.6394 - val_loss: 1313.1436\n",
      "Epoch 17/30\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 654.9072 - val_loss: 565.0717\n",
      "Epoch 18/30\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 806.9534 - val_loss: 514.6276\n",
      "Epoch 19/30\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 773.3825 - val_loss: 630.9061\n",
      "Epoch 20/30\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 707.0736 - val_loss: 828.9668\n",
      "Epoch 21/30\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 667.2994 - val_loss: 878.1767\n",
      "Epoch 22/30\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 770.4496 - val_loss: 565.9088\n",
      "Epoch 23/30\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 673.2052 - val_loss: 616.3856\n",
      "Epoch 24/30\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 673.5971 - val_loss: 466.9697\n",
      "Epoch 25/30\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 554.3376 - val_loss: 599.1617\n",
      "Epoch 26/30\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 729.9128 - val_loss: 1108.5622\n",
      "Epoch 27/30\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 670.5698 - val_loss: 574.0862\n",
      "Epoch 28/30\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 675.5506 - val_loss: 1210.4061\n",
      "Epoch 29/30\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 718.1459 - val_loss: 1104.8977\n",
      "Epoch 30/30\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 705.5690 - val_loss: 1066.4650\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ffa946a6ba8>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
