{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I referred to the following webpages for the implementation.\n",
    "- Implementation of Transformer<br>\n",
    "https://qiita.com/halhorn/items/c91497522be27bde17ce<br>\n",
    "https://github.com/kpot/keras-transformer/tree/master/keras_transformer<br>\n",
    "https://github.com/Lsdefine/attention-is-all-you-need-keras<br>\n",
    "- Usage of \"\\_\\_call\\_\\_\" method<br>\n",
    "https://qiita.com/kyo-bad/items/439d8cc3a0424c45214a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Activation, Layer, Embedding, Input, Reshape, Lambda, Add\n",
    "from keras import backend as K\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.utils import plot_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 8000\n",
    "d_model = 512\n",
    "MAX_LEN = 716\n",
    "class_num = 9\n",
    "PAD_ID = 0\n",
    "warmup_steps = 4000\n",
    "NUM_TRAIN = 5893\n",
    "NUM_TEST = 1474\n",
    "batch_size = 16\n",
    "epochs = 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(example):\n",
    "    features = tf.parse_single_example(\n",
    "        example,\n",
    "        features={\n",
    "            \"X\": tf.FixedLenFeature([MAX_LEN], dtype=tf.float32),\n",
    "            \"Y\": tf.FixedLenFeature((class_num,), dtype=tf.float32)\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    X = features[\"X\"]\n",
    "    Y = features[\"Y\"]\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterator(tfrecord_path, data_len):\n",
    "    dataset = tf.data.TFRecordDataset([tfrecord_path]).map(parse)\n",
    "    dataset = dataset.repeat(-1).batch(data_len)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    \n",
    "    X, Y = iterator.get_next()\n",
    "    X = tf.reshape(X, [-1, MAX_LEN])\n",
    "    \n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.11 s, sys: 1.38 s, total: 3.49 s\n",
      "Wall time: 2.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "x_train, y_train = iterator(\"train_transformer_with_sp.tfrecord\", NUM_TRAIN)\n",
    "x_test, y_test = iterator(\"test_transformer_with_sp.tfrecord\", NUM_TEST)\n",
    "\n",
    "x_train = tf.Session().run(x_train)\n",
    "y_train = tf.Session().run(y_train)\n",
    "x_test = tf.Session().run(x_test)\n",
    "y_test = tf.Session().run(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5893, 716), (5893, 9), (1474, 716), (1474, 9))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention():\n",
    "    ## hidden_dim has to be multiples of head_num\n",
    "    def __init__(self, max_len, hidden_dim=512, head_num=8, dropout_rate=0.1, *args, **kwargs):\n",
    "        self.max_len = max_len\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.head_num = head_num\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.q_dense_layer = Dense(hidden_dim, use_bias=False)\n",
    "        self.k_dense_layer = Dense(hidden_dim, use_bias=False)\n",
    "        self.v_dense_layer = Dense(hidden_dim, use_bias=False)\n",
    "        self.output_dense_layer = Dense(hidden_dim, use_bias=False)\n",
    "        self.attention_dropout_layer = Dropout(dropout_rate)\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        def reshape(x):\n",
    "            x = tf.reshape(x, [-1, self.max_len, self.head_num, self.hidden_dim // self.head_num])\n",
    "            return tf.transpose(x, [0, 2, 1, 3])\n",
    "        \n",
    "        out = Lambda(reshape)(x)\n",
    "        return out\n",
    "    \n",
    "    def combine_heads(self, heads):\n",
    "        def reshape(x):\n",
    "            heads = tf.transpose(x, [0, 2, 1, 3])\n",
    "            return tf.reshape(x, [-1, self.max_len, self.hidden_dim])\n",
    "        \n",
    "        out = Lambda(reshape)(heads)\n",
    "        return out\n",
    "        \n",
    "    def __call__(self, query, memory):\n",
    "        #two arguments of query and memory are already encoded as embedded vectors for all words\n",
    "        q = self.q_dense_layer(query)\n",
    "        k = self.k_dense_layer(memory)\n",
    "        v = self.v_dense_layer(memory)\n",
    "        \n",
    "        q = self.split_heads(q)\n",
    "        k = self.split_heads(k)\n",
    "        v = self.split_heads(v)\n",
    "        \n",
    "        #for scaled dot-product\n",
    "        depth_inside_each_head = self.hidden_dim // self.head_num\n",
    "        q = Lambda(lambda x: x * (depth_inside_each_head ** -0.5))(q)\n",
    "        \n",
    "        #q.shape = (batch_size, head_num, query_len, emb_dim)\n",
    "        #k.shape = (batch_size, head_num, memory_len, emb_dim)\n",
    "        #batch_dot(q, k).shape = (batch_size, head_num, query_len, memory_len)\n",
    "        score = Lambda(lambda x: K.batch_dot(x[0], x[1], axes=[3, 3]))([q, k])\n",
    "        normalized_score = Activation(\"softmax\")(score)\n",
    "        normalized_score = self.attention_dropout_layer(normalized_score)\n",
    "        \n",
    "        #normalized_score.shape = (batch_size, head_num, query_length, memory_length)\n",
    "        #v.shape = (batch_size, head_num, memory_length, depth)\n",
    "        #attention_weighted_output.shape = (batch_size, head_num, query_length, depth)\n",
    "        attention_weighted_output = Lambda(lambda x: K.batch_dot(x[0], x[1], axes=[3, 2]))([normalized_score, v])\n",
    "        attention_weighted_output = self.combine_heads(attention_weighted_output)\n",
    "        return self.output_dense_layer(attention_weighted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SlefAttention class inherits MultiheadAttention class so that it can make query and memory come from the same source.\n",
    "class SelfAttention(MultiheadAttention):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def __call__(self, query):\n",
    "        return super().__call__(query, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForwardNetwork():\n",
    "    \n",
    "    def __init__(self, hidden_dim, dropout_rate, *args, **kwargs):\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.first_dense_layer = Dense(hidden_dim*4, use_bias=True, activation=\"relu\")\n",
    "        self.second_dense_layer = Dense(hidden_dim, use_bias=True, activation=\"linear\")\n",
    "        self.dropout_layer = Dropout(dropout_rate)\n",
    "        \n",
    "    def __call__(self, inputs):\n",
    "        # make the network more flexible to learn for the first dense layer(non-linear transformation is used),\n",
    "        # and put the network back into the same hidden dim as original(linear transformation is used)\n",
    "        x = self.first_dense_layer(inputs)\n",
    "        x = self.dropout_layer(x)\n",
    "        return self.second_dense_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(Layer):\n",
    "    def __init__(self, axis=-1, **kwargs):\n",
    "        self.axis = axis\n",
    "        super(LayerNormalization, self).__init__(**kwargs)\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config[\"axis\"] = self.axis\n",
    "        return config\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        hidden_dim = input_shape[-1]\n",
    "        self.scale = self.add_weight(\"layer_norm_scale\", shape=[hidden_dim],\n",
    "                                    initializer=\"ones\")\n",
    "        self.shift = self.add_weight(\"layer_norm_shift\", shape=[hidden_dim],\n",
    "                                    initializer=\"zeros\")\n",
    "        super(LayerNormalization, self).build(input_shape)\n",
    "        \n",
    "    def call(self, inputs, epsilon=1e-6):\n",
    "        mean = K.mean(inputs, axis=[-1], keepdims=True)\n",
    "        variance = K.var(inputs, axis=[-1], keepdims=True)\n",
    "        normalized_inputs = (inputs - mean) / (K.sqrt(variance) + epsilon)\n",
    "        return normalized_inputs * self.scale + self.shift\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreLayerNormPostResidualConnectionWrapper():\n",
    "    def __init__(self, layer, dropout_rate, *args, **kwargs):\n",
    "        self.layer = layer\n",
    "        self.layer_norm = LayerNormalization()\n",
    "        self.dropout_layer = Dropout(dropout_rate)\n",
    "        \n",
    "    def __call__(self, inputs, *args, **kwargs):\n",
    "        x = self.layer_norm(inputs)\n",
    "        x = self.layer(x)\n",
    "        outputs = self.dropout_layer(x)\n",
    "        results = Add()([inputs, outputs])\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddPositionalEncoding(Layer): \n",
    "    def call(self, inputs):\n",
    "        data_type = inputs.dtype\n",
    "        batch_size, max_len, emb_dim = tf.unstack(tf.shape(inputs))\n",
    "        # i is from 0 to 255 when emb_dim is 512\n",
    "        #so the doubled_i is from 0 to 510\n",
    "        doubled_i = K.arange(emb_dim) // 2 * 2\n",
    "        exponent = K.tile(K.expand_dims(doubled_i, 0), [max_len, 1])\n",
    "        denominator_matrix = K.pow(10000.0, K.cast(exponent / emb_dim, data_type))\n",
    "        \n",
    "        # since cos(x) = sin(x + π/2), we convert the series of [sin, cos, sin, cos, ...]\n",
    "        # into [sin, sin, sin, sin, ...]\n",
    "        to_convert = K.cast(K.arange(emb_dim) % 2, data_type) * math.pi / 2\n",
    "        convert_matrix = K.tile(tf.expand_dims(to_convert, 0), [max_len, 1])\n",
    "        \n",
    "        seq_pos = K.arange(max_len)\n",
    "        numerator_matrix = K.cast(K.tile(K.expand_dims(seq_pos, 1), [1, emb_dim]), data_type)\n",
    "        \n",
    "        positinal_encoding = K.sin(numerator_matrix / denominator_matrix + convert_matrix)\n",
    "        batched_positional_encoding = K.tile(K.expand_dims(positinal_encoding, 0), [batch_size, 1, 1])\n",
    "        return inputs + batched_positional_encoding\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeZeroPads(Layer):\n",
    "    def __init__(self, seq_len, vocab_size, emb_dim, data_type=\"float32\", *args, **kwargs):\n",
    "        self.emb_dim = emb_dim\n",
    "        super(MakeZeroPads, self).__init__(*args, **kwargs)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        mask_for_pads = tf.to_float(tf.not_equal(inputs, PAD_ID))\n",
    "        pads_masked_embedding = inputs * mask_for_pads\n",
    "        return pads_masked_embedding * (self.emb_dim ** 0.5)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder():\n",
    "    def __init__(self, vocab_size, max_len, stack_num, head_num, emb_dim, dropout_rate, *args, **kwargs):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_len = max_len\n",
    "        self.stack_num = stack_num\n",
    "        self.head_num = head_num\n",
    "        self.emb_dim = emb_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.embedding_layer = Embedding(self.vocab_size,\n",
    "                           self.emb_dim,\n",
    "                           embeddings_initializer=RandomNormal(mean=0.0, stddev=self.emb_dim**-0.5)\n",
    "                          )\n",
    "        self.make_zero_pads_layer = MakeZeroPads(self.max_len, vocab_size, emb_dim)\n",
    "        self.add_pos_enc_layer = AddPositionalEncoding()\n",
    "        self.input_dropout_layer = Dropout(dropout_rate)\n",
    "        \n",
    "        self.attention_block_list = []\n",
    "        for _ in range(stack_num):\n",
    "            self_attention_layer = SelfAttention(self.max_len, self.emb_dim, self.head_num, self.dropout_rate)\n",
    "            pffn_layer = PositionwiseFeedForwardNetwork(self.emb_dim, self.dropout_rate)\n",
    "            self.attention_block_list.append([\n",
    "                PreLayerNormPostResidualConnectionWrapper(self_attention_layer, dropout_rate),\n",
    "                PreLayerNormPostResidualConnectionWrapper(pffn_layer, dropout_rate)\n",
    "            ])\n",
    "        self.output_layer_norm = LayerNormalization()\n",
    "        \n",
    "    def __call__(self, inputs):\n",
    "        x = self.embedding_layer(inputs)\n",
    "        x = self.make_zero_pads_layer(x)\n",
    "        x = self.add_pos_enc_layer(x)\n",
    "        x = self.input_dropout_layer(x)\n",
    "        \n",
    "        for i, set_of_layers_list in enumerate(self.attention_block_list):\n",
    "            self_attention_layer, pffn_layer = tuple(set_of_layers_list)\n",
    "            x = self_attention_layer(x)\n",
    "            x = pffn_layer(x)\n",
    "            \n",
    "        return self.output_layer_norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 716)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 716, 512)     4096000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "make_zero_pads_1 (MakeZeroPads) (None, 716, 512)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_positional_encoding_1 (AddP (None, 716, 512)     0           make_zero_pads_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 716, 512)     0           add_positional_encoding_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 716, 512)     1024        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 716, 512)     262144      layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 8, 716, 64)   0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 716, 512)     262144      layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 8, 716, 64)   0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 8, 716, 64)   0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 8, 716, 716)  0           lambda_4[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 8, 716, 716)  0           lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 716, 512)     262144      layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 8, 716, 716)  0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 8, 716, 64)   0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 8, 716, 64)   0           dropout_2[0][0]                  \n",
      "                                                                 lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 716, 512)     0           lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 716, 512)     262144      lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 716, 512)     0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 716, 512)     0           dropout_1[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_2 (LayerNor (None, 716, 512)     1024        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 716, 2048)    1050624     layer_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 716, 2048)    0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 716, 512)     1049088     dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 716, 512)     0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 716, 512)     0           add_1[0][0]                      \n",
      "                                                                 dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 716, 512)     1024        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 716, 512)     262144      layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 8, 716, 64)   0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 716, 512)     262144      layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 8, 716, 64)   0           lambda_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 8, 716, 64)   0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 8, 716, 716)  0           lambda_11[0][0]                  \n",
      "                                                                 lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 8, 716, 716)  0           lambda_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 716, 512)     262144      layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 8, 716, 716)  0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 8, 716, 64)   0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 8, 716, 64)   0           dropout_6[0][0]                  \n",
      "                                                                 lambda_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_14 (Lambda)              (None, 716, 512)     0           lambda_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 716, 512)     262144      lambda_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 716, 512)     0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 716, 512)     0           add_2[0][0]                      \n",
      "                                                                 dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_4 (LayerNor (None, 716, 512)     1024        add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 716, 2048)    1050624     layer_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 716, 2048)    0           dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 716, 512)     1049088     dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 716, 512)     0           dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 716, 512)     0           add_3[0][0]                      \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_5 (LayerNor (None, 716, 512)     1024        add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 716, 512)     262144      layer_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              (None, 8, 716, 64)   0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 716, 512)     262144      layer_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_18 (Lambda)              (None, 8, 716, 64)   0           lambda_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 8, 716, 64)   0           dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_19 (Lambda)              (None, 8, 716, 716)  0           lambda_18[0][0]                  \n",
      "                                                                 lambda_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 8, 716, 716)  0           lambda_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 716, 512)     262144      layer_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 8, 716, 716)  0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_17 (Lambda)              (None, 8, 716, 64)   0           dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_20 (Lambda)              (None, 8, 716, 64)   0           dropout_10[0][0]                 \n",
      "                                                                 lambda_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_21 (Lambda)              (None, 716, 512)     0           lambda_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 716, 512)     262144      lambda_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 716, 512)     0           dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 716, 512)     0           add_4[0][0]                      \n",
      "                                                                 dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_6 (LayerNor (None, 716, 512)     1024        add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 716, 2048)    1050624     layer_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 716, 2048)    0           dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 716, 512)     1049088     dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 716, 512)     0           dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 716, 512)     0           add_5[0][0]                      \n",
      "                                                                 dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_7 (LayerNor (None, 716, 512)     1024        add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 716, 512)     262144      layer_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_22 (Lambda)              (None, 8, 716, 64)   0           dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 716, 512)     262144      layer_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_25 (Lambda)              (None, 8, 716, 64)   0           lambda_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_23 (Lambda)              (None, 8, 716, 64)   0           dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_26 (Lambda)              (None, 8, 716, 716)  0           lambda_25[0][0]                  \n",
      "                                                                 lambda_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 8, 716, 716)  0           lambda_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 716, 512)     262144      layer_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 8, 716, 716)  0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_24 (Lambda)              (None, 8, 716, 64)   0           dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_27 (Lambda)              (None, 8, 716, 64)   0           dropout_14[0][0]                 \n",
      "                                                                 lambda_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_28 (Lambda)              (None, 716, 512)     0           lambda_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 716, 512)     262144      lambda_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 716, 512)     0           dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 716, 512)     0           add_6[0][0]                      \n",
      "                                                                 dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_8 (LayerNor (None, 716, 512)     1024        add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 716, 2048)    1050624     layer_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 716, 2048)    0           dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 716, 512)     1049088     dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 716, 512)     0           dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 716, 512)     0           add_7[0][0]                      \n",
      "                                                                 dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_9 (LayerNor (None, 716, 512)     1024        add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 716, 512)     262144      layer_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_29 (Lambda)              (None, 8, 716, 64)   0           dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 716, 512)     262144      layer_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_32 (Lambda)              (None, 8, 716, 64)   0           lambda_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_30 (Lambda)              (None, 8, 716, 64)   0           dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_33 (Lambda)              (None, 8, 716, 716)  0           lambda_32[0][0]                  \n",
      "                                                                 lambda_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 8, 716, 716)  0           lambda_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 716, 512)     262144      layer_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 8, 716, 716)  0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_31 (Lambda)              (None, 8, 716, 64)   0           dense_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_34 (Lambda)              (None, 8, 716, 64)   0           dropout_18[0][0]                 \n",
      "                                                                 lambda_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_35 (Lambda)              (None, 716, 512)     0           lambda_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 716, 512)     262144      lambda_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 716, 512)     0           dense_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 716, 512)     0           add_8[0][0]                      \n",
      "                                                                 dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_10 (LayerNo (None, 716, 512)     1024        add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 716, 2048)    1050624     layer_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 716, 2048)    0           dense_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 716, 512)     1049088     dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 716, 512)     0           dense_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 716, 512)     0           add_9[0][0]                      \n",
      "                                                                 dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_11 (LayerNo (None, 716, 512)     1024        add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 716, 512)     262144      layer_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_36 (Lambda)              (None, 8, 716, 64)   0           dense_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 716, 512)     262144      layer_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_39 (Lambda)              (None, 8, 716, 64)   0           lambda_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_37 (Lambda)              (None, 8, 716, 64)   0           dense_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_40 (Lambda)              (None, 8, 716, 716)  0           lambda_39[0][0]                  \n",
      "                                                                 lambda_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 8, 716, 716)  0           lambda_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 716, 512)     262144      layer_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 8, 716, 716)  0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_38 (Lambda)              (None, 8, 716, 64)   0           dense_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_41 (Lambda)              (None, 8, 716, 64)   0           dropout_22[0][0]                 \n",
      "                                                                 lambda_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_42 (Lambda)              (None, 716, 512)     0           lambda_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 716, 512)     262144      lambda_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 716, 512)     0           dense_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 716, 512)     0           add_10[0][0]                     \n",
      "                                                                 dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_12 (LayerNo (None, 716, 512)     1024        add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 716, 2048)    1050624     layer_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 716, 2048)    0           dense_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, 716, 512)     1049088     dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 716, 512)     0           dense_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 716, 512)     0           add_11[0][0]                     \n",
      "                                                                 dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_13 (LayerNo (None, 716, 512)     1024        add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_43 (Lambda)              (None, 512)          0           layer_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, 9)            4617        lambda_43[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 23,003,657\n",
      "Trainable params: 23,003,657\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Transformer classification model\n",
    "inputs = Input((MAX_LEN,))\n",
    "transformer_encoder = Encoder(vocab_size=vocab_size, stack_num=6, head_num=8, emb_dim=512, dropout_rate=0.1, max_len=MAX_LEN)\n",
    "encoder_output = transformer_encoder(inputs)\n",
    "#Since the task is text classification, we just need the first token for each sample\n",
    "summarized_vecs = Lambda(lambda x: x[:, 0, :])(encoder_output)\n",
    "outputs = Dense(class_num)(summarized_vecs)\n",
    "model = Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file=\"transformer_encoder.png\", show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning rate for Adam\n",
    "class LRSchedulerPerStep(Callback):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        self.d_model = d_model\n",
    "        self.warmup_step = warmup_steps\n",
    "        self.step_num = 0\n",
    "        \n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        self.step_num += 1\n",
    "        updated_lr = self.d_model * min(self.step_num ** (-0.5), self.step_num * (self.warmup_step ** (-1.5)))\n",
    "        K.set_value(self.model.optimizer.lr, updated_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scheduler = LRSchedulerPerStep(d_model, warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About customized loss function\n",
    "https://stackoverflow.com/questions/50063613/add-loss-function-in-keras<br>\n",
    "https://github.com/kpot/keras-transformer/blob/b9d4e76c535c0c62cadc73e37416e4dc18b635ca/keras_transformer/bert.py#L212<br>\n",
    "https://github.com/tensorflow/models/blob/b9ef963d1e84da0bb9c0a6039457c39a699ea149/official/transformer/v2/metrics.py#L47<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# smoothed loss function\n",
    "class SmoothedLossSparseCategoricalXEntropy:\n",
    "    def __init__(self, smoothing, class_num):\n",
    "        self.smoothing = smoothing\n",
    "        self.class_num = class_num\n",
    "        \n",
    "    def __call__(self, y_true, y_pred):\n",
    "        confidence = 1.0 - self.smoothing\n",
    "        low_confidence = (1.0 - confidence) / tf.cast(self.class_num - 1, tf.float32)\n",
    "        smoothed_labels = tf.one_hot(\n",
    "            tf.cast(y_true, tf.int32),\n",
    "            depth=self.class_num,\n",
    "            on_value=confidence,\n",
    "            off_value=low_confidence\n",
    "        )\n",
    "        xentropy = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            logits=y_pred,\n",
    "            labels=smoothed_labels\n",
    "        )\n",
    "        \n",
    "        lowest_loss = -(confidence * tf.log(confidence) + \n",
    "                       tf.cast(self.class_num -1, tf.float32) * low_confidence * tf.log(low_confidence + 1e-20))\n",
    "        final_loss = xentropy - lowest_loss\n",
    "        return final_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5893,), (1474,))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = np.argmax(y_train, axis=-1)\n",
    "y_test = np.argmax(y_test, axis=-1)\n",
    "y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=opt, loss=SmoothedLossSparseCategoricalXEntropy(smoothing=0.1, class_num=class_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=opt, loss=categorical_crossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5893 samples, validate on 1474 samples\n",
      "Epoch 1/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.9624 - val_loss: 32.3749\n",
      "Epoch 2/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.7330 - val_loss: 13.7559\n",
      "Epoch 3/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.5895 - val_loss: 16.3020\n",
      "Epoch 4/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 25.0843 - val_loss: 57.8977\n",
      "Epoch 5/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 26.2714 - val_loss: 44.8746\n",
      "Epoch 6/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.4282 - val_loss: 15.9615\n",
      "Epoch 7/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 25.4451 - val_loss: 19.3501\n",
      "Epoch 8/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.4507 - val_loss: 17.5041\n",
      "Epoch 9/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 28.1414 - val_loss: 14.6088\n",
      "Epoch 10/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.7490 - val_loss: 14.7437\n",
      "Epoch 11/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.3740 - val_loss: 29.9071\n",
      "Epoch 12/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.3158 - val_loss: 28.9556\n",
      "Epoch 13/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 25.2636 - val_loss: 25.0236\n",
      "Epoch 14/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.3843 - val_loss: 91.3709\n",
      "Epoch 15/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.3516 - val_loss: 34.6443\n",
      "Epoch 16/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.0668 - val_loss: 13.0277\n",
      "Epoch 17/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 28.7611 - val_loss: 31.1536\n",
      "Epoch 18/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.2643 - val_loss: 30.9244\n",
      "Epoch 19/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.9670 - val_loss: 18.1992\n",
      "Epoch 20/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.3522 - val_loss: 35.9847\n",
      "Epoch 21/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 26.8542 - val_loss: 60.8041\n",
      "Epoch 22/1500\n",
      "5893/5893 [==============================] - 133s 22ms/step - loss: 23.9519 - val_loss: 25.1996\n",
      "Epoch 23/1500\n",
      "5893/5893 [==============================] - 133s 22ms/step - loss: 24.8719 - val_loss: 27.4879\n",
      "Epoch 24/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.9286 - val_loss: 30.6980\n",
      "Epoch 25/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.7668 - val_loss: 17.2803\n",
      "Epoch 26/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.0631 - val_loss: 16.4899\n",
      "Epoch 27/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.8939 - val_loss: 126.8105\n",
      "Epoch 28/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 26.7486 - val_loss: 37.7619\n",
      "Epoch 29/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 26.6747 - val_loss: 89.1015\n",
      "Epoch 30/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.8429 - val_loss: 38.1295\n",
      "Epoch 31/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.9206 - val_loss: 21.9588\n",
      "Epoch 32/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.0594 - val_loss: 14.6349\n",
      "Epoch 33/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.2835 - val_loss: 34.2982\n",
      "Epoch 34/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.5680 - val_loss: 28.8224\n",
      "Epoch 35/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.7046 - val_loss: 22.9985\n",
      "Epoch 36/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 26.9901 - val_loss: 30.4677\n",
      "Epoch 37/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.6188 - val_loss: 15.4432\n",
      "Epoch 38/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.0728 - val_loss: 18.3843\n",
      "Epoch 39/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.0536 - val_loss: 22.6473\n",
      "Epoch 40/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.5770 - val_loss: 39.3790\n",
      "Epoch 41/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 25.2736 - val_loss: 43.5564\n",
      "Epoch 42/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 26.1263 - val_loss: 19.9951\n",
      "Epoch 43/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.5125 - val_loss: 30.4005\n",
      "Epoch 44/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.8587 - val_loss: 36.0820\n",
      "Epoch 45/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 29.2282 - val_loss: 20.6087\n",
      "Epoch 46/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.5548 - val_loss: 22.9232\n",
      "Epoch 47/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.7281 - val_loss: 33.9022\n",
      "Epoch 48/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.9967 - val_loss: 63.4773\n",
      "Epoch 49/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.4748 - val_loss: 31.7582\n",
      "Epoch 50/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.3886 - val_loss: 31.8544\n",
      "Epoch 51/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.9058 - val_loss: 33.8836\n",
      "Epoch 52/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.2377 - val_loss: 31.4573\n",
      "Epoch 53/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.0755 - val_loss: 27.0172\n",
      "Epoch 54/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.5270 - val_loss: 25.9946\n",
      "Epoch 55/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.9237 - val_loss: 49.1974\n",
      "Epoch 56/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.1838 - val_loss: 16.6623\n",
      "Epoch 57/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.3234 - val_loss: 46.1659\n",
      "Epoch 58/1500\n",
      "5893/5893 [==============================] - 133s 22ms/step - loss: 24.1293 - val_loss: 34.9229\n",
      "Epoch 59/1500\n",
      "5893/5893 [==============================] - 133s 22ms/step - loss: 21.4728 - val_loss: 35.9462\n",
      "Epoch 60/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.6855 - val_loss: 22.8342\n",
      "Epoch 61/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.2064 - val_loss: 30.1829\n",
      "Epoch 62/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.8027 - val_loss: 33.2439\n",
      "Epoch 63/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.4038 - val_loss: 18.5951\n",
      "Epoch 64/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.3347 - val_loss: 16.7270\n",
      "Epoch 65/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.7085 - val_loss: 23.6907\n",
      "Epoch 66/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.5220 - val_loss: 26.7918\n",
      "Epoch 67/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.1444 - val_loss: 17.0673\n",
      "Epoch 68/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.5693 - val_loss: 23.3579\n",
      "Epoch 69/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 25.6513 - val_loss: 36.6798\n",
      "Epoch 70/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 25.3025 - val_loss: 37.3211\n",
      "Epoch 71/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.8494 - val_loss: 30.5609\n",
      "Epoch 72/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.1067 - val_loss: 24.4997\n",
      "Epoch 73/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 25.8808 - val_loss: 22.0231\n",
      "Epoch 74/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.7976 - val_loss: 65.3165\n",
      "Epoch 75/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.8964 - val_loss: 25.4310\n",
      "Epoch 76/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.9266 - val_loss: 39.4387\n",
      "Epoch 77/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.8916 - val_loss: 30.0352\n",
      "Epoch 78/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.8790 - val_loss: 25.6923\n",
      "Epoch 79/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.0707 - val_loss: 22.3227\n",
      "Epoch 80/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.2750 - val_loss: 22.3666\n",
      "Epoch 81/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.2315 - val_loss: 18.0864\n",
      "Epoch 82/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.2467 - val_loss: 35.6662\n",
      "Epoch 83/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.2531 - val_loss: 32.9786\n",
      "Epoch 84/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.7749 - val_loss: 24.2302\n",
      "Epoch 85/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.7956 - val_loss: 20.4108\n",
      "Epoch 86/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.3497 - val_loss: 28.4019\n",
      "Epoch 87/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.0836 - val_loss: 26.1901\n",
      "Epoch 88/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 27.0129 - val_loss: 29.2053\n",
      "Epoch 89/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.8517 - val_loss: 17.7593\n",
      "Epoch 90/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.3194 - val_loss: 38.7968\n",
      "Epoch 91/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.9528 - val_loss: 30.5946\n",
      "Epoch 92/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.2540 - val_loss: 45.0248\n",
      "Epoch 93/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.8462 - val_loss: 20.4934\n",
      "Epoch 94/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.8320 - val_loss: 20.7364\n",
      "Epoch 95/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 27.5372 - val_loss: 19.6358\n",
      "Epoch 96/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.2508 - val_loss: 37.7869\n",
      "Epoch 97/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.3858 - val_loss: 42.5639\n",
      "Epoch 98/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.2642 - val_loss: 21.0003\n",
      "Epoch 99/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.5161 - val_loss: 37.3091\n",
      "Epoch 100/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.4624 - val_loss: 30.3678\n",
      "Epoch 101/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 26.8480 - val_loss: 41.4050\n",
      "Epoch 102/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.5363 - val_loss: 18.8558\n",
      "Epoch 103/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.7763 - val_loss: 43.2604\n",
      "Epoch 104/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.0700 - val_loss: 29.3882\n",
      "Epoch 105/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.3371 - val_loss: 27.9701\n",
      "Epoch 106/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.4279 - val_loss: 30.3473\n",
      "Epoch 107/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 25.4108 - val_loss: 34.7064\n",
      "Epoch 108/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.8722 - val_loss: 25.7709\n",
      "Epoch 109/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.7470 - val_loss: 28.4945\n",
      "Epoch 110/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 28.3270 - val_loss: 49.0161\n",
      "Epoch 111/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.3674 - val_loss: 29.3646\n",
      "Epoch 112/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.5516 - val_loss: 34.6561\n",
      "Epoch 113/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.9435 - val_loss: 39.2758\n",
      "Epoch 114/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.9762 - val_loss: 16.7422\n",
      "Epoch 115/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.7210 - val_loss: 15.7812\n",
      "Epoch 116/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.0394 - val_loss: 21.3821\n",
      "Epoch 117/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.0166 - val_loss: 13.1299\n",
      "Epoch 118/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.6311 - val_loss: 20.6068\n",
      "Epoch 119/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.1504 - val_loss: 25.3685\n",
      "Epoch 120/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.8914 - val_loss: 37.0337\n",
      "Epoch 121/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 25.1631 - val_loss: 28.8602\n",
      "Epoch 122/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.8461 - val_loss: 43.1345\n",
      "Epoch 123/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.4298 - val_loss: 26.2212\n",
      "Epoch 124/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.2292 - val_loss: 19.9612\n",
      "Epoch 125/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.2091 - val_loss: 20.4996\n",
      "Epoch 126/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 26.8016 - val_loss: 44.9554\n",
      "Epoch 127/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.4949 - val_loss: 32.7222\n",
      "Epoch 128/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.8963 - val_loss: 31.1450\n",
      "Epoch 129/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.2035 - val_loss: 20.5861\n",
      "Epoch 130/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 25.4496 - val_loss: 53.0328\n",
      "Epoch 131/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.7738 - val_loss: 62.3331\n",
      "Epoch 132/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.7682 - val_loss: 41.4129\n",
      "Epoch 133/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.6326 - val_loss: 14.5542\n",
      "Epoch 134/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.7109 - val_loss: 30.6382\n",
      "Epoch 135/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.2444 - val_loss: 12.3352\n",
      "Epoch 136/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.9280 - val_loss: 50.6436\n",
      "Epoch 137/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.6311 - val_loss: 18.6383\n",
      "Epoch 138/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.8494 - val_loss: 29.6769\n",
      "Epoch 139/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.9636 - val_loss: 46.7061\n",
      "Epoch 140/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.6937 - val_loss: 39.0015\n",
      "Epoch 141/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.6180 - val_loss: 21.4722\n",
      "Epoch 142/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.7208 - val_loss: 34.7963\n",
      "Epoch 143/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.9654 - val_loss: 16.8446\n",
      "Epoch 144/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.2168 - val_loss: 20.1299\n",
      "Epoch 145/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.1895 - val_loss: 37.7847\n",
      "Epoch 146/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.6905 - val_loss: 35.0435\n",
      "Epoch 147/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.6090 - val_loss: 32.5217\n",
      "Epoch 148/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.9291 - val_loss: 24.1406\n",
      "Epoch 149/1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.2066 - val_loss: 14.3196\n",
      "Epoch 150/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 25.3517 - val_loss: 17.1741\n",
      "Epoch 151/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.2528 - val_loss: 15.3556\n",
      "Epoch 152/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 28.5929 - val_loss: 25.5474\n",
      "Epoch 153/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.7322 - val_loss: 10.4462\n",
      "Epoch 154/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.2833 - val_loss: 15.7129\n",
      "Epoch 155/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.6267 - val_loss: 21.5379\n",
      "Epoch 156/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 26.2649 - val_loss: 51.8340\n",
      "Epoch 157/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 28.3988 - val_loss: 14.4192\n",
      "Epoch 158/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.6289 - val_loss: 15.5432\n",
      "Epoch 159/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.8053 - val_loss: 31.7106\n",
      "Epoch 160/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.9825 - val_loss: 38.3800\n",
      "Epoch 161/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.4945 - val_loss: 37.6178\n",
      "Epoch 162/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.8336 - val_loss: 33.9972\n",
      "Epoch 163/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.0417 - val_loss: 19.1353\n",
      "Epoch 164/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.7234 - val_loss: 18.4059\n",
      "Epoch 165/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.0507 - val_loss: 14.7774\n",
      "Epoch 166/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.8439 - val_loss: 14.5256\n",
      "Epoch 167/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.6241 - val_loss: 32.3112\n",
      "Epoch 168/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.4535 - val_loss: 24.2708\n",
      "Epoch 169/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 28.4491 - val_loss: 19.4438\n",
      "Epoch 170/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.5701 - val_loss: 51.1735\n",
      "Epoch 171/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.7235 - val_loss: 24.3176\n",
      "Epoch 172/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.7660 - val_loss: 27.9841\n",
      "Epoch 173/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.8132 - val_loss: 43.0628\n",
      "Epoch 174/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.0624 - val_loss: 35.5512\n",
      "Epoch 175/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.7323 - val_loss: 13.9411\n",
      "Epoch 176/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.9568 - val_loss: 21.6491\n",
      "Epoch 177/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.1912 - val_loss: 48.7120\n",
      "Epoch 178/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 25.0236 - val_loss: 24.1890\n",
      "Epoch 179/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.6412 - val_loss: 33.7222\n",
      "Epoch 180/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.7095 - val_loss: 41.3964\n",
      "Epoch 181/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 26.2177 - val_loss: 17.6187\n",
      "Epoch 182/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.4916 - val_loss: 25.4332\n",
      "Epoch 183/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.3877 - val_loss: 24.9122\n",
      "Epoch 184/1500\n",
      "5893/5893 [==============================] - 134s 23ms/step - loss: 23.0354 - val_loss: 22.7275\n",
      "Epoch 185/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.2585 - val_loss: 26.4970\n",
      "Epoch 186/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.9868 - val_loss: 13.7744\n",
      "Epoch 187/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.8128 - val_loss: 25.9016\n",
      "Epoch 188/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.1677 - val_loss: 34.6000\n",
      "Epoch 189/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.7710 - val_loss: 38.4709\n",
      "Epoch 190/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.0591 - val_loss: 20.5065\n",
      "Epoch 191/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.5629 - val_loss: 46.9710\n",
      "Epoch 192/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.8996 - val_loss: 31.3530\n",
      "Epoch 193/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.0825 - val_loss: 19.9125\n",
      "Epoch 194/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.0196 - val_loss: 37.6717\n",
      "Epoch 195/1500\n",
      "5893/5893 [==============================] - 134s 23ms/step - loss: 20.0498 - val_loss: 29.2304\n",
      "Epoch 196/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.9473 - val_loss: 19.5422\n",
      "Epoch 197/1500\n",
      "5893/5893 [==============================] - 134s 23ms/step - loss: 19.8656 - val_loss: 39.9818\n",
      "Epoch 198/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 25.9981 - val_loss: 34.9991\n",
      "Epoch 199/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.4622 - val_loss: 41.6054\n",
      "Epoch 200/1500\n",
      "5893/5893 [==============================] - 134s 23ms/step - loss: 20.9239 - val_loss: 58.7772\n",
      "Epoch 201/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.5740 - val_loss: 28.6331\n",
      "Epoch 202/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.4022 - val_loss: 17.0617\n",
      "Epoch 203/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.4196 - val_loss: 44.1599\n",
      "Epoch 204/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.6120 - val_loss: 31.2779\n",
      "Epoch 205/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.4305 - val_loss: 22.6429\n",
      "Epoch 206/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.0547 - val_loss: 12.8941\n",
      "Epoch 207/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.9131 - val_loss: 29.7488\n",
      "Epoch 208/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.4789 - val_loss: 27.6366\n",
      "Epoch 209/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.1994 - val_loss: 43.0677\n",
      "Epoch 210/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.7044 - val_loss: 33.7171\n",
      "Epoch 211/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.0484 - val_loss: 62.3391\n",
      "Epoch 212/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.5535 - val_loss: 46.0231\n",
      "Epoch 213/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.5019 - val_loss: 90.0296\n",
      "Epoch 214/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.4837 - val_loss: 24.2662\n",
      "Epoch 215/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 25.7831 - val_loss: 14.4451\n",
      "Epoch 216/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.9265 - val_loss: 19.3196\n",
      "Epoch 217/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.7750 - val_loss: 37.0560\n",
      "Epoch 218/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.1297 - val_loss: 14.8990\n",
      "Epoch 219/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.2142 - val_loss: 25.0592\n",
      "Epoch 220/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.4708 - val_loss: 17.6680\n",
      "Epoch 221/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.1169 - val_loss: 10.9759\n",
      "Epoch 222/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.9930 - val_loss: 35.8137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 223/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.2639 - val_loss: 28.3779\n",
      "Epoch 224/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.7945 - val_loss: 25.9436\n",
      "Epoch 225/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.8163 - val_loss: 23.9911\n",
      "Epoch 226/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.4190 - val_loss: 18.8584\n",
      "Epoch 227/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.7762 - val_loss: 26.3570\n",
      "Epoch 228/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.7953 - val_loss: 21.7330\n",
      "Epoch 229/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.1314 - val_loss: 43.0261\n",
      "Epoch 230/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.8534 - val_loss: 20.8807\n",
      "Epoch 231/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.6378 - val_loss: 21.7428\n",
      "Epoch 232/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.0718 - val_loss: 20.5803\n",
      "Epoch 233/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.7702 - val_loss: 24.9387\n",
      "Epoch 234/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.0169 - val_loss: 25.6572\n",
      "Epoch 235/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.5498 - val_loss: 85.6082\n",
      "Epoch 236/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.7912 - val_loss: 19.1067\n",
      "Epoch 237/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.2216 - val_loss: 25.9278\n",
      "Epoch 238/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.0481 - val_loss: 33.5949\n",
      "Epoch 239/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.8974 - val_loss: 31.5190\n",
      "Epoch 240/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.8731 - val_loss: 24.2455\n",
      "Epoch 241/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.1483 - val_loss: 22.0719\n",
      "Epoch 242/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.5950 - val_loss: 22.4901\n",
      "Epoch 243/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.9000 - val_loss: 20.3618\n",
      "Epoch 244/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.5394 - val_loss: 13.4164\n",
      "Epoch 245/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.0573 - val_loss: 44.9119\n",
      "Epoch 246/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.0668 - val_loss: 14.2892\n",
      "Epoch 247/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.5417 - val_loss: 18.3542\n",
      "Epoch 248/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.3114 - val_loss: 18.0663\n",
      "Epoch 249/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.4610 - val_loss: 15.3947\n",
      "Epoch 250/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.7222 - val_loss: 30.8097\n",
      "Epoch 251/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.6658 - val_loss: 14.9338\n",
      "Epoch 252/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.4508 - val_loss: 21.4226\n",
      "Epoch 253/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.7996 - val_loss: 17.3318\n",
      "Epoch 254/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.4543 - val_loss: 23.5627\n",
      "Epoch 255/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.3156 - val_loss: 15.2261\n",
      "Epoch 256/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.5134 - val_loss: 21.7769\n",
      "Epoch 257/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.1956 - val_loss: 15.5300\n",
      "Epoch 258/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.4057 - val_loss: 31.8706\n",
      "Epoch 259/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.1955 - val_loss: 30.5695\n",
      "Epoch 260/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.5643 - val_loss: 23.4771\n",
      "Epoch 261/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.6230 - val_loss: 17.0278\n",
      "Epoch 262/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.5818 - val_loss: 15.4292\n",
      "Epoch 263/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.6442 - val_loss: 20.1165\n",
      "Epoch 264/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.9840 - val_loss: 24.0393\n",
      "Epoch 265/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.5904 - val_loss: 24.8260\n",
      "Epoch 266/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.8732 - val_loss: 22.4122\n",
      "Epoch 267/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.7403 - val_loss: 17.0711\n",
      "Epoch 268/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 25.8369 - val_loss: 38.9496\n",
      "Epoch 269/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.1165 - val_loss: 48.5087\n",
      "Epoch 270/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.1502 - val_loss: 28.5336\n",
      "Epoch 271/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 27.1281 - val_loss: 33.8399\n",
      "Epoch 272/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.1196 - val_loss: 29.2358\n",
      "Epoch 273/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.5974 - val_loss: 26.8105\n",
      "Epoch 274/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.5766 - val_loss: 34.5562\n",
      "Epoch 275/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.1719 - val_loss: 44.0545\n",
      "Epoch 276/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.0767 - val_loss: 25.2280\n",
      "Epoch 277/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.5715 - val_loss: 16.6393\n",
      "Epoch 278/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.3388 - val_loss: 34.4249\n",
      "Epoch 279/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.8238 - val_loss: 50.7889\n",
      "Epoch 280/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.5075 - val_loss: 19.4194\n",
      "Epoch 281/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.9811 - val_loss: 47.0544\n",
      "Epoch 282/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.9396 - val_loss: 27.2363\n",
      "Epoch 283/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.8185 - val_loss: 52.6769\n",
      "Epoch 284/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.7762 - val_loss: 20.8948\n",
      "Epoch 285/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.6464 - val_loss: 20.5972\n",
      "Epoch 286/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.9602 - val_loss: 23.9213\n",
      "Epoch 287/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.6573 - val_loss: 29.5164\n",
      "Epoch 288/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.7951 - val_loss: 60.1262\n",
      "Epoch 289/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.3778 - val_loss: 10.3020\n",
      "Epoch 290/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.7530 - val_loss: 23.3636\n",
      "Epoch 291/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.4500 - val_loss: 27.9750\n",
      "Epoch 292/1500\n",
      "5893/5893 [==============================] - 133s 22ms/step - loss: 19.4308 - val_loss: 20.2378\n",
      "Epoch 293/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.7151 - val_loss: 38.4308\n",
      "Epoch 294/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.4535 - val_loss: 18.1513\n",
      "Epoch 295/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.6908 - val_loss: 19.6784\n",
      "Epoch 296/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.0573 - val_loss: 39.7744\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 297/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.3048 - val_loss: 41.1021\n",
      "Epoch 298/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.2418 - val_loss: 27.4605\n",
      "Epoch 299/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.3997 - val_loss: 31.3019\n",
      "Epoch 300/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.5544 - val_loss: 25.5013\n",
      "Epoch 301/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 28.7757 - val_loss: 22.8332\n",
      "Epoch 302/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.9393 - val_loss: 28.9404\n",
      "Epoch 303/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.7574 - val_loss: 35.4959\n",
      "Epoch 304/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.1593 - val_loss: 22.4182\n",
      "Epoch 305/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.3106 - val_loss: 14.3136\n",
      "Epoch 306/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.5604 - val_loss: 27.8209\n",
      "Epoch 307/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.2742 - val_loss: 20.9911\n",
      "Epoch 308/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.5589 - val_loss: 27.0966\n",
      "Epoch 309/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.2608 - val_loss: 10.3929\n",
      "Epoch 310/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.4886 - val_loss: 22.9901\n",
      "Epoch 311/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.5300 - val_loss: 76.1302\n",
      "Epoch 312/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.3927 - val_loss: 27.1252\n",
      "Epoch 313/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.7064 - val_loss: 17.4326\n",
      "Epoch 314/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.5795 - val_loss: 19.9273\n",
      "Epoch 315/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.8216 - val_loss: 23.5959\n",
      "Epoch 316/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.7514 - val_loss: 24.5413\n",
      "Epoch 317/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.0290 - val_loss: 15.0501\n",
      "Epoch 318/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.8318 - val_loss: 12.3425\n",
      "Epoch 319/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.5228 - val_loss: 19.4526\n",
      "Epoch 320/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.8879 - val_loss: 22.0719\n",
      "Epoch 321/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.6210 - val_loss: 16.0882\n",
      "Epoch 322/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.7269 - val_loss: 18.3544\n",
      "Epoch 323/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.7984 - val_loss: 19.4716\n",
      "Epoch 324/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.7916 - val_loss: 24.5833\n",
      "Epoch 325/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.6533 - val_loss: 22.4748\n",
      "Epoch 326/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.7470 - val_loss: 15.6998\n",
      "Epoch 327/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.4549 - val_loss: 24.2104\n",
      "Epoch 328/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 26.2742 - val_loss: 11.8338\n",
      "Epoch 329/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.9349 - val_loss: 20.8503\n",
      "Epoch 330/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.6041 - val_loss: 43.8791\n",
      "Epoch 331/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 29.1829 - val_loss: 18.3126\n",
      "Epoch 332/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.7837 - val_loss: 29.6723\n",
      "Epoch 333/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.0334 - val_loss: 44.8643\n",
      "Epoch 334/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.8454 - val_loss: 24.5239\n",
      "Epoch 335/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.6796 - val_loss: 31.6865\n",
      "Epoch 336/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 25.7839 - val_loss: 21.9478\n",
      "Epoch 337/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.3791 - val_loss: 23.0710\n",
      "Epoch 338/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.2006 - val_loss: 53.1048\n",
      "Epoch 339/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.1526 - val_loss: 21.0506\n",
      "Epoch 340/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.7884 - val_loss: 19.8562\n",
      "Epoch 341/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.3567 - val_loss: 12.9501\n",
      "Epoch 342/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.5255 - val_loss: 35.4302\n",
      "Epoch 343/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 15.5164 - val_loss: 61.6544\n",
      "Epoch 344/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.0782 - val_loss: 19.5551\n",
      "Epoch 345/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.5538 - val_loss: 14.8039\n",
      "Epoch 346/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.1304 - val_loss: 21.2456\n",
      "Epoch 347/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.9580 - val_loss: 32.6686\n",
      "Epoch 348/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.2457 - val_loss: 35.9835\n",
      "Epoch 349/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.0036 - val_loss: 24.1553\n",
      "Epoch 350/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.1638 - val_loss: 49.4727\n",
      "Epoch 351/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.4217 - val_loss: 34.0166\n",
      "Epoch 352/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.7510 - val_loss: 23.5811\n",
      "Epoch 353/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.4109 - val_loss: 22.2096\n",
      "Epoch 354/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.7252 - val_loss: 29.7754\n",
      "Epoch 355/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.4171 - val_loss: 23.3200\n",
      "Epoch 356/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.2365 - val_loss: 20.2129\n",
      "Epoch 357/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.9443 - val_loss: 25.5875\n",
      "Epoch 358/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.0857 - val_loss: 32.3644\n",
      "Epoch 359/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.9661 - val_loss: 44.4301\n",
      "Epoch 360/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.6082 - val_loss: 22.2116\n",
      "Epoch 361/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.8126 - val_loss: 13.9252\n",
      "Epoch 362/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.8992 - val_loss: 18.1353\n",
      "Epoch 363/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.7091 - val_loss: 34.3381\n",
      "Epoch 364/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.5692 - val_loss: 18.8433\n",
      "Epoch 365/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.3294 - val_loss: 22.2369\n",
      "Epoch 366/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.0524 - val_loss: 22.7206\n",
      "Epoch 367/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.7357 - val_loss: 22.6858\n",
      "Epoch 368/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.9449 - val_loss: 10.0204\n",
      "Epoch 369/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.7493 - val_loss: 22.1357\n",
      "Epoch 370/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.3102 - val_loss: 21.2741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 371/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.6533 - val_loss: 26.4738\n",
      "Epoch 372/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.0885 - val_loss: 26.1284\n",
      "Epoch 373/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.8461 - val_loss: 14.1141\n",
      "Epoch 374/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.5052 - val_loss: 58.0295\n",
      "Epoch 375/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.9390 - val_loss: 23.5564\n",
      "Epoch 376/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.3871 - val_loss: 24.9405\n",
      "Epoch 377/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.6619 - val_loss: 18.1376\n",
      "Epoch 378/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.5846 - val_loss: 13.3663\n",
      "Epoch 379/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.9823 - val_loss: 18.5773\n",
      "Epoch 380/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.7863 - val_loss: 28.2060\n",
      "Epoch 381/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.9642 - val_loss: 20.0445\n",
      "Epoch 382/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.3277 - val_loss: 19.4740\n",
      "Epoch 383/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.6214 - val_loss: 9.0031\n",
      "Epoch 384/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.3861 - val_loss: 50.5364\n",
      "Epoch 385/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.0588 - val_loss: 21.8477\n",
      "Epoch 386/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.2794 - val_loss: 41.9810\n",
      "Epoch 387/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.3251 - val_loss: 45.1924\n",
      "Epoch 388/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.1837 - val_loss: 30.5541\n",
      "Epoch 389/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.8039 - val_loss: 26.6223\n",
      "Epoch 390/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.8541 - val_loss: 46.6032\n",
      "Epoch 391/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.6053 - val_loss: 31.5330\n",
      "Epoch 392/1500\n",
      "5893/5893 [==============================] - 133s 22ms/step - loss: 16.8409 - val_loss: 51.2245\n",
      "Epoch 393/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.2931 - val_loss: 17.8068\n",
      "Epoch 394/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.3677 - val_loss: 23.9900\n",
      "Epoch 395/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.2845 - val_loss: 22.5202\n",
      "Epoch 396/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.8176 - val_loss: 40.5459\n",
      "Epoch 397/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.4061 - val_loss: 14.2521\n",
      "Epoch 398/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.3373 - val_loss: 32.3065\n",
      "Epoch 399/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.5740 - val_loss: 32.3555\n",
      "Epoch 400/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.5146 - val_loss: 27.5453\n",
      "Epoch 401/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.5921 - val_loss: 33.0584\n",
      "Epoch 402/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.5231 - val_loss: 28.7683\n",
      "Epoch 403/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.3830 - val_loss: 26.4074\n",
      "Epoch 404/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.1788 - val_loss: 20.7547\n",
      "Epoch 405/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.6294 - val_loss: 19.1832\n",
      "Epoch 406/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.9804 - val_loss: 26.0014\n",
      "Epoch 407/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.6508 - val_loss: 18.5201\n",
      "Epoch 408/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.0309 - val_loss: 20.5346\n",
      "Epoch 409/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.4239 - val_loss: 23.1644\n",
      "Epoch 410/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.6672 - val_loss: 31.6490\n",
      "Epoch 411/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.7605 - val_loss: 15.5689\n",
      "Epoch 412/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.6681 - val_loss: 24.8962\n",
      "Epoch 413/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.4746 - val_loss: 42.8212\n",
      "Epoch 414/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.1064 - val_loss: 27.8239\n",
      "Epoch 415/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.9549 - val_loss: 24.8596\n",
      "Epoch 416/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 15.7560 - val_loss: 27.1876\n",
      "Epoch 417/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.4454 - val_loss: 20.9157\n",
      "Epoch 418/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.5120 - val_loss: 21.6247\n",
      "Epoch 419/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.7356 - val_loss: 32.2343\n",
      "Epoch 420/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.9508 - val_loss: 20.2201\n",
      "Epoch 421/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.0955 - val_loss: 67.1143\n",
      "Epoch 422/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 25.1559 - val_loss: 30.5089\n",
      "Epoch 423/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.1679 - val_loss: 20.6807\n",
      "Epoch 424/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.3142 - val_loss: 16.7711\n",
      "Epoch 425/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.2605 - val_loss: 28.8744\n",
      "Epoch 426/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.8564 - val_loss: 21.0345\n",
      "Epoch 427/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.3114 - val_loss: 15.5522\n",
      "Epoch 428/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.5546 - val_loss: 21.2250\n",
      "Epoch 429/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.7105 - val_loss: 41.8840\n",
      "Epoch 430/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.3700 - val_loss: 14.0412\n",
      "Epoch 431/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.8994 - val_loss: 13.9977\n",
      "Epoch 432/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.1848 - val_loss: 46.7499\n",
      "Epoch 433/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.3704 - val_loss: 47.4486\n",
      "Epoch 434/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.0712 - val_loss: 21.0869\n",
      "Epoch 435/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.9814 - val_loss: 26.0752\n",
      "Epoch 436/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.9403 - val_loss: 18.5591\n",
      "Epoch 437/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.4102 - val_loss: 31.2150\n",
      "Epoch 438/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.4677 - val_loss: 23.4040\n",
      "Epoch 439/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.6265 - val_loss: 16.5731\n",
      "Epoch 440/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.7339 - val_loss: 62.3760\n",
      "Epoch 441/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.7923 - val_loss: 29.2385\n",
      "Epoch 442/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.4176 - val_loss: 28.1233\n",
      "Epoch 443/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.4704 - val_loss: 27.3302\n",
      "Epoch 444/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.1989 - val_loss: 20.6072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 445/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.6476 - val_loss: 19.2025\n",
      "Epoch 446/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.9693 - val_loss: 17.2687\n",
      "Epoch 447/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.7955 - val_loss: 38.3026\n",
      "Epoch 448/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.0274 - val_loss: 26.7623\n",
      "Epoch 449/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.9853 - val_loss: 28.0459\n",
      "Epoch 450/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.2764 - val_loss: 25.4735\n",
      "Epoch 451/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.0983 - val_loss: 24.5112\n",
      "Epoch 452/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.8735 - val_loss: 35.5192\n",
      "Epoch 453/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.3848 - val_loss: 13.6190\n",
      "Epoch 454/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.6001 - val_loss: 15.4518\n",
      "Epoch 455/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.6318 - val_loss: 30.7722\n",
      "Epoch 456/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.5847 - val_loss: 35.9957\n",
      "Epoch 457/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.6787 - val_loss: 25.8690\n",
      "Epoch 458/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.6152 - val_loss: 7.8645\n",
      "Epoch 459/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.0449 - val_loss: 52.6924\n",
      "Epoch 460/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.8093 - val_loss: 40.3492\n",
      "Epoch 461/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.4732 - val_loss: 27.6433\n",
      "Epoch 462/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.3631 - val_loss: 34.3687\n",
      "Epoch 463/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.6199 - val_loss: 22.1884\n",
      "Epoch 464/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.8468 - val_loss: 29.6881\n",
      "Epoch 465/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.0303 - val_loss: 22.2321\n",
      "Epoch 466/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.7982 - val_loss: 16.9804\n",
      "Epoch 467/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.0799 - val_loss: 36.1949\n",
      "Epoch 468/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.8660 - val_loss: 35.0889\n",
      "Epoch 469/1500\n",
      "5893/5893 [==============================] - 134s 23ms/step - loss: 18.3286 - val_loss: 21.5306\n",
      "Epoch 470/1500\n",
      "5893/5893 [==============================] - 134s 23ms/step - loss: 19.6567 - val_loss: 16.2555\n",
      "Epoch 471/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.4219 - val_loss: 27.4145\n",
      "Epoch 472/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.7395 - val_loss: 47.1032\n",
      "Epoch 473/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.0776 - val_loss: 26.7087\n",
      "Epoch 474/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.4243 - val_loss: 38.8721\n",
      "Epoch 475/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.0528 - val_loss: 46.9395\n",
      "Epoch 476/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.3701 - val_loss: 20.4859\n",
      "Epoch 477/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.2086 - val_loss: 34.0491\n",
      "Epoch 478/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.6905 - val_loss: 31.5848\n",
      "Epoch 479/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.3461 - val_loss: 51.9757\n",
      "Epoch 480/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 25.8069 - val_loss: 18.9979\n",
      "Epoch 481/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.2164 - val_loss: 24.5761\n",
      "Epoch 482/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.1493 - val_loss: 22.0780\n",
      "Epoch 483/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.6482 - val_loss: 21.4608\n",
      "Epoch 484/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.8134 - val_loss: 20.1174\n",
      "Epoch 485/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.7259 - val_loss: 27.1420\n",
      "Epoch 486/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.7280 - val_loss: 31.6115\n",
      "Epoch 487/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.6595 - val_loss: 36.2782\n",
      "Epoch 488/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.3618 - val_loss: 10.9257\n",
      "Epoch 489/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.5929 - val_loss: 22.3223\n",
      "Epoch 490/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.2067 - val_loss: 19.2778\n",
      "Epoch 491/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.8440 - val_loss: 18.6307\n",
      "Epoch 492/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.2634 - val_loss: 16.9716\n",
      "Epoch 493/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.9939 - val_loss: 27.8684\n",
      "Epoch 494/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.6930 - val_loss: 34.1584\n",
      "Epoch 495/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.8170 - val_loss: 38.0243\n",
      "Epoch 496/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.6134 - val_loss: 35.6542\n",
      "Epoch 497/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.5361 - val_loss: 36.3916\n",
      "Epoch 498/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.8748 - val_loss: 23.2016\n",
      "Epoch 499/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.3317 - val_loss: 22.0750\n",
      "Epoch 500/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.0017 - val_loss: 25.7836\n",
      "Epoch 501/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.2199 - val_loss: 57.5742\n",
      "Epoch 502/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.5883 - val_loss: 19.9278\n",
      "Epoch 503/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.2916 - val_loss: 28.4043\n",
      "Epoch 504/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.7299 - val_loss: 27.5695\n",
      "Epoch 505/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.4543 - val_loss: 25.3460\n",
      "Epoch 506/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.8478 - val_loss: 17.7123\n",
      "Epoch 507/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.1367 - val_loss: 23.1442\n",
      "Epoch 508/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.5889 - val_loss: 24.2921\n",
      "Epoch 509/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.4842 - val_loss: 22.6162\n",
      "Epoch 510/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.8894 - val_loss: 27.4139\n",
      "Epoch 511/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.7487 - val_loss: 18.2791\n",
      "Epoch 512/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.3439 - val_loss: 23.5867\n",
      "Epoch 513/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.1834 - val_loss: 17.4639\n",
      "Epoch 514/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.2765 - val_loss: 26.4281\n",
      "Epoch 515/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.1667 - val_loss: 12.2677\n",
      "Epoch 516/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.7668 - val_loss: 24.8757\n",
      "Epoch 517/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.4633 - val_loss: 23.8724\n",
      "Epoch 518/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.8387 - val_loss: 28.1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 519/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.6885 - val_loss: 21.2219\n",
      "Epoch 520/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.0785 - val_loss: 19.7507\n",
      "Epoch 521/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.1204 - val_loss: 38.1986\n",
      "Epoch 522/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.5774 - val_loss: 22.3591\n",
      "Epoch 523/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.7330 - val_loss: 33.5343\n",
      "Epoch 524/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.1535 - val_loss: 14.7110\n",
      "Epoch 525/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.9629 - val_loss: 17.2825\n",
      "Epoch 526/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.0762 - val_loss: 29.5293\n",
      "Epoch 527/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 15.6767 - val_loss: 32.6733\n",
      "Epoch 528/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.8402 - val_loss: 24.0440\n",
      "Epoch 529/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.8957 - val_loss: 24.1420\n",
      "Epoch 530/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.9497 - val_loss: 19.8889\n",
      "Epoch 531/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.8494 - val_loss: 44.1739\n",
      "Epoch 532/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.6632 - val_loss: 19.1443\n",
      "Epoch 533/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.8071 - val_loss: 13.4212\n",
      "Epoch 534/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.3885 - val_loss: 15.0142\n",
      "Epoch 535/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.2529 - val_loss: 15.3675\n",
      "Epoch 536/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.9283 - val_loss: 20.0824\n",
      "Epoch 537/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.1323 - val_loss: 28.9104\n",
      "Epoch 538/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.8318 - val_loss: 16.0439\n",
      "Epoch 539/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.1319 - val_loss: 14.8436\n",
      "Epoch 540/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.3699 - val_loss: 21.4308\n",
      "Epoch 541/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.3873 - val_loss: 17.1957\n",
      "Epoch 542/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.3497 - val_loss: 19.0673\n",
      "Epoch 543/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.0469 - val_loss: 17.2167\n",
      "Epoch 544/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.5785 - val_loss: 26.9025\n",
      "Epoch 545/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.6969 - val_loss: 20.2202\n",
      "Epoch 546/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.0377 - val_loss: 17.7829\n",
      "Epoch 547/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.3654 - val_loss: 26.1238\n",
      "Epoch 548/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.5124 - val_loss: 31.3207\n",
      "Epoch 549/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.4509 - val_loss: 18.6645\n",
      "Epoch 550/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.9158 - val_loss: 19.0277\n",
      "Epoch 551/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.0186 - val_loss: 27.2745\n",
      "Epoch 552/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.6160 - val_loss: 20.2533\n",
      "Epoch 553/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.6828 - val_loss: 24.0698\n",
      "Epoch 554/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.2701 - val_loss: 35.7323\n",
      "Epoch 555/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.8718 - val_loss: 19.1190\n",
      "Epoch 556/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.1133 - val_loss: 11.0662\n",
      "Epoch 557/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.2118 - val_loss: 26.6744\n",
      "Epoch 558/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.2902 - val_loss: 14.1737\n",
      "Epoch 559/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.7906 - val_loss: 24.1425\n",
      "Epoch 560/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.0886 - val_loss: 11.8750\n",
      "Epoch 561/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.5017 - val_loss: 33.0191\n",
      "Epoch 562/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.4352 - val_loss: 18.7358\n",
      "Epoch 563/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.5588 - val_loss: 24.3186\n",
      "Epoch 564/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.2378 - val_loss: 32.2452\n",
      "Epoch 565/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.6517 - val_loss: 16.5335\n",
      "Epoch 566/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.8144 - val_loss: 15.1428\n",
      "Epoch 567/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.0029 - val_loss: 30.3609\n",
      "Epoch 568/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.1185 - val_loss: 50.2683\n",
      "Epoch 569/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.7742 - val_loss: 25.6040\n",
      "Epoch 570/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.2546 - val_loss: 19.7890\n",
      "Epoch 571/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.6891 - val_loss: 57.2713\n",
      "Epoch 572/1500\n",
      "5893/5893 [==============================] - 134s 23ms/step - loss: 19.4753 - val_loss: 34.2738\n",
      "Epoch 573/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.3311 - val_loss: 16.3694\n",
      "Epoch 574/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.8188 - val_loss: 17.5229\n",
      "Epoch 575/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 15.5258 - val_loss: 10.7657\n",
      "Epoch 576/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.3163 - val_loss: 17.0521\n",
      "Epoch 577/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.3144 - val_loss: 18.1295\n",
      "Epoch 578/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 24.3309 - val_loss: 12.1057\n",
      "Epoch 579/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.2614 - val_loss: 28.4142\n",
      "Epoch 580/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.0491 - val_loss: 11.9930\n",
      "Epoch 581/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.6622 - val_loss: 13.5504\n",
      "Epoch 582/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.7565 - val_loss: 33.6188\n",
      "Epoch 583/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.6719 - val_loss: 28.0069\n",
      "Epoch 584/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.0582 - val_loss: 30.7560\n",
      "Epoch 585/1500\n",
      "5893/5893 [==============================] - 133s 22ms/step - loss: 16.4367 - val_loss: 15.7048\n",
      "Epoch 586/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.9737 - val_loss: 25.1562\n",
      "Epoch 587/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.1079 - val_loss: 8.4133\n",
      "Epoch 588/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.7813 - val_loss: 19.3080\n",
      "Epoch 589/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.4180 - val_loss: 14.6935\n",
      "Epoch 590/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.8542 - val_loss: 17.3218\n",
      "Epoch 591/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.2594 - val_loss: 23.2221\n",
      "Epoch 592/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.1963 - val_loss: 14.7192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 593/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 15.6430 - val_loss: 43.8835\n",
      "Epoch 594/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.7378 - val_loss: 19.8704\n",
      "Epoch 595/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.8682 - val_loss: 12.2706\n",
      "Epoch 596/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.9721 - val_loss: 15.6397\n",
      "Epoch 597/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.6423 - val_loss: 16.6528\n",
      "Epoch 598/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.0407 - val_loss: 14.2894\n",
      "Epoch 599/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.5584 - val_loss: 26.5086\n",
      "Epoch 600/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.2116 - val_loss: 13.1457\n",
      "Epoch 601/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 15.7128 - val_loss: 18.2923\n",
      "Epoch 602/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.5836 - val_loss: 20.3142\n",
      "Epoch 603/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.8429 - val_loss: 10.3745\n",
      "Epoch 604/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.9192 - val_loss: 17.2419\n",
      "Epoch 605/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.3047 - val_loss: 20.5838\n",
      "Epoch 606/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.1172 - val_loss: 9.1200\n",
      "Epoch 607/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.9586 - val_loss: 19.7005\n",
      "Epoch 608/1500\n",
      "5893/5893 [==============================] - 133s 22ms/step - loss: 19.5785 - val_loss: 19.7746\n",
      "Epoch 609/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.0183 - val_loss: 12.9805\n",
      "Epoch 610/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.7662 - val_loss: 23.2193\n",
      "Epoch 611/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.8924 - val_loss: 17.9801\n",
      "Epoch 612/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.5385 - val_loss: 24.3080\n",
      "Epoch 613/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.1092 - val_loss: 35.4769\n",
      "Epoch 614/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.9588 - val_loss: 17.1052\n",
      "Epoch 615/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.6131 - val_loss: 17.5389\n",
      "Epoch 616/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.0404 - val_loss: 31.9555\n",
      "Epoch 617/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.8933 - val_loss: 21.5412\n",
      "Epoch 618/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.1245 - val_loss: 25.5049\n",
      "Epoch 619/1500\n",
      "5893/5893 [==============================] - 133s 22ms/step - loss: 18.5667 - val_loss: 67.5673\n",
      "Epoch 620/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.9759 - val_loss: 24.4422\n",
      "Epoch 621/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.0667 - val_loss: 33.6990\n",
      "Epoch 622/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.6422 - val_loss: 26.6286\n",
      "Epoch 623/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.8704 - val_loss: 30.0379\n",
      "Epoch 624/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.5925 - val_loss: 16.1836\n",
      "Epoch 625/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.9213 - val_loss: 21.6350\n",
      "Epoch 626/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.0446 - val_loss: 25.8929\n",
      "Epoch 627/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.7143 - val_loss: 11.3246\n",
      "Epoch 628/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.2802 - val_loss: 12.3253\n",
      "Epoch 629/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.7823 - val_loss: 29.8966\n",
      "Epoch 630/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.4028 - val_loss: 11.2336\n",
      "Epoch 631/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.0145 - val_loss: 9.9886\n",
      "Epoch 632/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.9029 - val_loss: 18.5064\n",
      "Epoch 633/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.5646 - val_loss: 15.5152\n",
      "Epoch 634/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.5147 - val_loss: 34.4104\n",
      "Epoch 635/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.0102 - val_loss: 17.1113\n",
      "Epoch 636/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.5723 - val_loss: 17.6572\n",
      "Epoch 637/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.5372 - val_loss: 21.7045\n",
      "Epoch 638/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.4300 - val_loss: 20.2099\n",
      "Epoch 639/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.8213 - val_loss: 16.9868\n",
      "Epoch 640/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.4676 - val_loss: 28.2003\n",
      "Epoch 641/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.4381 - val_loss: 16.6715\n",
      "Epoch 642/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.4015 - val_loss: 16.3933\n",
      "Epoch 643/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.4414 - val_loss: 17.6773\n",
      "Epoch 644/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.7182 - val_loss: 16.6969\n",
      "Epoch 645/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.9423 - val_loss: 10.7848\n",
      "Epoch 646/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.0047 - val_loss: 22.2163\n",
      "Epoch 647/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.7249 - val_loss: 16.0390\n",
      "Epoch 648/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.2456 - val_loss: 50.3287\n",
      "Epoch 649/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.8388 - val_loss: 21.0023\n",
      "Epoch 650/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.5223 - val_loss: 30.8690\n",
      "Epoch 651/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.6479 - val_loss: 42.0573\n",
      "Epoch 652/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.9981 - val_loss: 14.5994\n",
      "Epoch 653/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.7145 - val_loss: 29.4577\n",
      "Epoch 654/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.1034 - val_loss: 25.7646\n",
      "Epoch 655/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.0957 - val_loss: 18.5086\n",
      "Epoch 656/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 15.4945 - val_loss: 26.3665\n",
      "Epoch 657/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.9394 - val_loss: 18.5128\n",
      "Epoch 658/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.9503 - val_loss: 19.4531\n",
      "Epoch 659/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.6557 - val_loss: 15.3991\n",
      "Epoch 660/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.8060 - val_loss: 25.2843\n",
      "Epoch 661/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.3896 - val_loss: 21.3045\n",
      "Epoch 662/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.3455 - val_loss: 36.0940\n",
      "Epoch 663/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.0300 - val_loss: 8.6920\n",
      "Epoch 664/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.5231 - val_loss: 41.8465\n",
      "Epoch 665/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.6438 - val_loss: 38.5731\n",
      "Epoch 666/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.0926 - val_loss: 13.6940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 667/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.0326 - val_loss: 18.7542\n",
      "Epoch 668/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.7447 - val_loss: 23.2304\n",
      "Epoch 669/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.5712 - val_loss: 40.6428\n",
      "Epoch 670/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.3142 - val_loss: 29.2735\n",
      "Epoch 671/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.8980 - val_loss: 38.3935\n",
      "Epoch 672/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.3204 - val_loss: 15.5841\n",
      "Epoch 673/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.9696 - val_loss: 12.7203\n",
      "Epoch 674/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.0325 - val_loss: 8.3871\n",
      "Epoch 675/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.6565 - val_loss: 20.1963\n",
      "Epoch 676/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.4676 - val_loss: 17.4534\n",
      "Epoch 677/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.1516 - val_loss: 16.5310\n",
      "Epoch 678/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.7640 - val_loss: 25.7598\n",
      "Epoch 679/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.4676 - val_loss: 20.8331\n",
      "Epoch 680/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.4219 - val_loss: 11.5136\n",
      "Epoch 681/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.5267 - val_loss: 15.0077\n",
      "Epoch 682/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 23.4654 - val_loss: 15.6067\n",
      "Epoch 683/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.5446 - val_loss: 16.9569\n",
      "Epoch 684/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.1581 - val_loss: 18.7266\n",
      "Epoch 685/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 22.8233 - val_loss: 28.3270\n",
      "Epoch 686/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.6389 - val_loss: 20.7666\n",
      "Epoch 687/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.4245 - val_loss: 39.6192\n",
      "Epoch 688/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.9606 - val_loss: 14.6322\n",
      "Epoch 689/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.0966 - val_loss: 13.9182\n",
      "Epoch 690/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.0785 - val_loss: 18.6421\n",
      "Epoch 691/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.6766 - val_loss: 29.0042\n",
      "Epoch 692/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.8966 - val_loss: 15.6480\n",
      "Epoch 693/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.2517 - val_loss: 48.4327\n",
      "Epoch 694/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.8671 - val_loss: 28.6847\n",
      "Epoch 695/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 15.9673 - val_loss: 21.5229\n",
      "Epoch 696/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.8319 - val_loss: 10.5592\n",
      "Epoch 697/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.0781 - val_loss: 10.2999\n",
      "Epoch 698/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.0765 - val_loss: 15.8391\n",
      "Epoch 699/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.2115 - val_loss: 21.1853\n",
      "Epoch 700/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.7362 - val_loss: 17.7600\n",
      "Epoch 701/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.1763 - val_loss: 13.9882\n",
      "Epoch 702/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.4045 - val_loss: 23.0585\n",
      "Epoch 703/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 15.6025 - val_loss: 11.6695\n",
      "Epoch 704/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.3848 - val_loss: 13.5439\n",
      "Epoch 705/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.6631 - val_loss: 21.5944\n",
      "Epoch 706/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.5123 - val_loss: 14.1133\n",
      "Epoch 707/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.0817 - val_loss: 14.3118\n",
      "Epoch 708/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.4745 - val_loss: 21.9873\n",
      "Epoch 709/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.7886 - val_loss: 19.2758\n",
      "Epoch 710/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.1187 - val_loss: 17.8550\n",
      "Epoch 711/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.4887 - val_loss: 17.4910\n",
      "Epoch 712/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.9496 - val_loss: 17.3833\n",
      "Epoch 713/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.2049 - val_loss: 15.3173\n",
      "Epoch 714/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.9616 - val_loss: 14.7900\n",
      "Epoch 715/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.3122 - val_loss: 19.0622\n",
      "Epoch 716/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.3311 - val_loss: 19.6529\n",
      "Epoch 717/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.5414 - val_loss: 39.7565\n",
      "Epoch 718/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.4152 - val_loss: 30.8543\n",
      "Epoch 719/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.2651 - val_loss: 20.5475\n",
      "Epoch 720/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.5915 - val_loss: 39.1310\n",
      "Epoch 721/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.5368 - val_loss: 29.4527\n",
      "Epoch 722/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.3471 - val_loss: 57.1065\n",
      "Epoch 723/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.1771 - val_loss: 20.3760\n",
      "Epoch 724/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.6468 - val_loss: 25.9412\n",
      "Epoch 725/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.1230 - val_loss: 16.5712\n",
      "Epoch 726/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 15.3102 - val_loss: 22.9234\n",
      "Epoch 727/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 15.8771 - val_loss: 18.0990\n",
      "Epoch 728/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 15.6814 - val_loss: 24.8579\n",
      "Epoch 729/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.3290 - val_loss: 15.9811\n",
      "Epoch 730/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 15.2724 - val_loss: 13.0782\n",
      "Epoch 731/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.7395 - val_loss: 27.7716\n",
      "Epoch 732/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.0745 - val_loss: 22.2267\n",
      "Epoch 733/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.9627 - val_loss: 21.2946\n",
      "Epoch 734/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.3208 - val_loss: 30.7505\n",
      "Epoch 735/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.0736 - val_loss: 19.4683\n",
      "Epoch 736/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.9130 - val_loss: 20.6239\n",
      "Epoch 737/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.8661 - val_loss: 18.5711\n",
      "Epoch 738/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.9628 - val_loss: 18.0464\n",
      "Epoch 739/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.7480 - val_loss: 13.3238\n",
      "Epoch 740/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.4799 - val_loss: 16.3776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 741/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.3864 - val_loss: 23.2348\n",
      "Epoch 742/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 15.0170 - val_loss: 18.0799\n",
      "Epoch 743/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.9749 - val_loss: 24.5515\n",
      "Epoch 744/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.9259 - val_loss: 21.5562\n",
      "Epoch 745/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.4928 - val_loss: 20.9698\n",
      "Epoch 746/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.1134 - val_loss: 28.0166\n",
      "Epoch 747/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.8244 - val_loss: 19.1329\n",
      "Epoch 748/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.4559 - val_loss: 23.7582\n",
      "Epoch 749/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.9105 - val_loss: 16.3236\n",
      "Epoch 750/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.2887 - val_loss: 16.7924\n",
      "Epoch 751/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 15.3346 - val_loss: 33.8890\n",
      "Epoch 752/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.3926 - val_loss: 23.6659\n",
      "Epoch 753/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.2513 - val_loss: 33.6243\n",
      "Epoch 754/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.4840 - val_loss: 26.5031\n",
      "Epoch 755/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.9288 - val_loss: 14.9545\n",
      "Epoch 756/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.0134 - val_loss: 12.7983\n",
      "Epoch 757/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 13.8268 - val_loss: 22.8098\n",
      "Epoch 758/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.1691 - val_loss: 19.5138\n",
      "Epoch 759/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.1778 - val_loss: 23.8876\n",
      "Epoch 760/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.3347 - val_loss: 18.7513\n",
      "Epoch 761/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.7595 - val_loss: 39.6869\n",
      "Epoch 762/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.5120 - val_loss: 14.3274\n",
      "Epoch 763/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.9677 - val_loss: 18.5035\n",
      "Epoch 764/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.4805 - val_loss: 28.0366\n",
      "Epoch 765/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.9393 - val_loss: 29.2464\n",
      "Epoch 766/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.5331 - val_loss: 42.4918\n",
      "Epoch 767/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.6523 - val_loss: 14.9684\n",
      "Epoch 768/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 15.4601 - val_loss: 23.9886\n",
      "Epoch 769/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.2533 - val_loss: 33.0124\n",
      "Epoch 770/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 15.3552 - val_loss: 66.5509\n",
      "Epoch 771/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.5065 - val_loss: 20.6712\n",
      "Epoch 772/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.0022 - val_loss: 13.5554\n",
      "Epoch 773/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.1317 - val_loss: 9.8855\n",
      "Epoch 774/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.6439 - val_loss: 14.3163\n",
      "Epoch 775/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.3421 - val_loss: 29.8752\n",
      "Epoch 776/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.8693 - val_loss: 19.1532\n",
      "Epoch 777/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 15.6692 - val_loss: 22.6822\n",
      "Epoch 778/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.3352 - val_loss: 8.5608\n",
      "Epoch 779/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 15.8295 - val_loss: 38.3052\n",
      "Epoch 780/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.2795 - val_loss: 13.2247\n",
      "Epoch 781/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.0040 - val_loss: 12.0417\n",
      "Epoch 782/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.4017 - val_loss: 12.2548\n",
      "Epoch 783/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.0137 - val_loss: 30.1179\n",
      "Epoch 784/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.3192 - val_loss: 26.5742\n",
      "Epoch 785/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.8313 - val_loss: 27.2349\n",
      "Epoch 786/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 14.5862 - val_loss: 24.9218\n",
      "Epoch 787/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.1073 - val_loss: 32.8228\n",
      "Epoch 788/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.3140 - val_loss: 26.3562\n",
      "Epoch 789/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.6256 - val_loss: 11.4135\n",
      "Epoch 790/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.9992 - val_loss: 10.7220\n",
      "Epoch 791/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 15.6643 - val_loss: 22.0819\n",
      "Epoch 792/1500\n",
      "5893/5893 [==============================] - 134s 23ms/step - loss: 19.6071 - val_loss: 26.9460\n",
      "Epoch 793/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 14.9286 - val_loss: 15.4826\n",
      "Epoch 794/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.4931 - val_loss: 9.9762\n",
      "Epoch 795/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 14.6183 - val_loss: 32.8850\n",
      "Epoch 796/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.9881 - val_loss: 42.2755\n",
      "Epoch 797/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 21.0886 - val_loss: 29.0184\n",
      "Epoch 798/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.0803 - val_loss: 12.0173\n",
      "Epoch 799/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.3630 - val_loss: 23.3742\n",
      "Epoch 800/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 15.3989 - val_loss: 14.3895\n",
      "Epoch 801/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.4667 - val_loss: 12.0296\n",
      "Epoch 802/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.3192 - val_loss: 12.9139\n",
      "Epoch 803/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.1103 - val_loss: 17.0691\n",
      "Epoch 804/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.6502 - val_loss: 16.9412\n",
      "Epoch 805/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.9453 - val_loss: 18.3536\n",
      "Epoch 806/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 16.2606 - val_loss: 18.3975\n",
      "Epoch 807/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.0138 - val_loss: 30.8743\n",
      "Epoch 808/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.3720 - val_loss: 31.5765\n",
      "Epoch 809/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.7253 - val_loss: 35.9225\n",
      "Epoch 810/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 15.1560 - val_loss: 15.3689\n",
      "Epoch 811/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 15.6170 - val_loss: 18.2251\n",
      "Epoch 812/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 19.1493 - val_loss: 21.9220\n",
      "Epoch 813/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.2995 - val_loss: 18.6597\n",
      "Epoch 814/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 20.4103 - val_loss: 10.1197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 815/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.2793 - val_loss: 14.8239\n",
      "Epoch 816/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 18.3668 - val_loss: 26.8804\n",
      "Epoch 817/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.1494 - val_loss: 30.3411\n",
      "Epoch 818/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.4518 - val_loss: 16.0064\n",
      "Epoch 819/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 15.4407 - val_loss: 11.9828\n",
      "Epoch 820/1500\n",
      "5893/5893 [==============================] - 133s 23ms/step - loss: 17.4047 - val_loss: 23.0878\n",
      "Epoch 821/1500\n",
      "2192/5893 [==========>...................] - ETA: 1:19 - loss: 15.4437"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-9b38033024bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2719\u001b[0m                     \u001b[0;34m'In order to feed symbolic tensors to a Keras model '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2720\u001b[0m                     'in TensorFlow, you need tensorflow 1.8 or higher.')\n\u001b[0;32m-> 2721\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_legacy_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2691\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2693\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2694\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), callbacks=[lr_scheduler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
