{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I referred to the following webpages for the implementation.\n",
    "- Implementation of Transformer<br>\n",
    "https://qiita.com/halhorn/items/c91497522be27bde17ce<br>\n",
    "https://github.com/kpot/keras-transformer/tree/master/keras_transformer<br>\n",
    "https://github.com/Lsdefine/attention-is-all-you-need-keras<br>\n",
    "- Usage of \"\\_\\_call\\_\\_\" method<br>\n",
    "https://qiita.com/kyo-bad/items/439d8cc3a0424c45214a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Activation, Layer, Embedding, Input, Reshape, Lambda\n",
    "from keras import backend as K\n",
    "from keras.initializers import RandomNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention():\n",
    "    ## hidden_dim has to be multiples of head_num\n",
    "    def __init__(self, hidden_dim=512, head_num=8, dropout_rate=0.1, *args, **kwargs):\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.head_num = head_num\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.q_dense_layer = Dense(hidden_dim, use_bias=False, name=\"q_dense_layer\")\n",
    "        self.k_dense_layer = Dense(hidden_dim, use_bias=False, name=\"k_dense_layer\")\n",
    "        self.v_dense_layer = Dense(hidden_dim, use_bias=False, name=\"v_dense_layer\")\n",
    "        self.output_dense_layer = Dense(hidden_dim, use_bias=False, name=\"output_dense_layer\")\n",
    "        self.attention_dropout_layer = Dropout(dropout_rate, name=\"attention_dropout_layer\")\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        def reshape(x):\n",
    "            batch_size, max_len, hidden_dim = tf.unstack(tf.shape(x))\n",
    "            x = tf.reshape(x, [batch_size, max_len, self.head_num, self.hidden_dim // self.head_num])\n",
    "            return tf.transpose(x, [0, 2, 1, 3])\n",
    "        \n",
    "        out = Lambda(reshape)(x)\n",
    "        return out\n",
    "    \n",
    "    def combine_heads(self, heads):\n",
    "        def reshape(x):\n",
    "            batch_size, _, max_len, _ = tf.unstack(tf.shape(x))\n",
    "            heads = tf.transpose(x, [0, 2, 1, 3])\n",
    "            return tf.reshape(x, [batch_size, max_len, self.hidden_dim])\n",
    "        \n",
    "        out = Lambda(reshape)(heads)\n",
    "        return out\n",
    "        \n",
    "    def __call__(self, query, memory):\n",
    "        #two arguments of query and memory are already encoded as embedded vectors for all words\n",
    "        q = self.q_dense_layer(query)\n",
    "        k = self.k_dense_layer(memory)\n",
    "        v = self.v_dense_layer(memory)\n",
    "        \n",
    "        q = self.split_heads(q)\n",
    "        k = self.split_heads(k)\n",
    "        v = self.split_heads(v)\n",
    "        \n",
    "        #for scaled dot-product\n",
    "        depth_inside_each_head = self.hidden_dim // self.head_num\n",
    "        q = Lambda(lambda x: x * (depth_inside_each_head ** -0.5))(q)\n",
    "        \n",
    "        #q.shape = (batch_size, query_len, emb_dim)\n",
    "        #k.shape = (batch_size, memory_len, emb_dim)\n",
    "        #batch_dot(q, k).shape = (batch_size, query_len, memory_len)\n",
    "        score = Lambda(lambda x: K.batch_dot(x[0], x[1], axes=[2, 2]))([q, k])\n",
    "        normalized_score = Activation(\"softmax\")(score)\n",
    "        normalized_score = self.attention_dropout_layer(normalized_score)\n",
    "        #attention_weighted_output = tf.matmul(normalized_score, v)\n",
    "        \n",
    "        #normalized_score.shape = (batch_size, query_length, memory_length)\n",
    "        #v.shape = (batch_size, memory_length, depth)\n",
    "        #attention_weighted_output.shape = (batch_size, query_length, depth)\n",
    "        attention_weighted_output = Lambda(lambda x: K.batch_dot(x[0], x[1], axes=[2, 1]))([normalized_score, v])\n",
    "        attention_weighted_output = self.combine_heads(attention_weighted_output)\n",
    "        return self.output_dense_layer(attention_weighted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SlefAttention class inherits MultiheadAttention class so that it can make query and memory come from the same source.\n",
    "class SelfAttention(MultiheadAttention):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def __call__(self, query):\n",
    "        return super().__call__(query, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForwardNetwork():\n",
    "    \n",
    "    def __init__(self, hidden_dim, dropout_rate, *args, **kwargs):\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.first_dense_layer = Dense(hidden_dim*4, use_bias=True, activation=\"relu\", name=\"first_dense_layer\")\n",
    "        self.second_dense_layer = Dense(hidden_dim, use_bias=True, activation=\"linear\", name=\"second_dense_layer\")\n",
    "        self.dropout_layer = Dropout(dropout_rate, name=\"PFFN_dropout\")\n",
    "        \n",
    "    def __call__(self, inputs):\n",
    "        # make the network more flexible to learn for the first dense layer(non-linear transformation is used),\n",
    "        # and put the network back into the same hidden dim as original(linear transformation is used)\n",
    "        x = self.first_dense_layer(inputs)\n",
    "        x = self.dropout_layer(x)\n",
    "        return self.second_dense_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(Layer):\n",
    "    def __init__(self, axis=-1, **kwargs):\n",
    "        self.axis = axis\n",
    "        super(LayerNormalization, self).__init__(**kwargs)\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config[\"axis\"] = self.axis\n",
    "        return config\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        hidden_dim = input_shape[-1]\n",
    "        self.scale = self.add_weight(\"layer_norm_scale\", shape=[hidden_dim],\n",
    "                                    initializer=\"ones\")\n",
    "        self.shift = self.add_weight(\"layer_norm_shift\", shape=[hidden_dim],\n",
    "                                    initializer=\"zeros\")\n",
    "        super(LayerNormalization, self).build(input_shape)\n",
    "        \n",
    "    def call(self, inputs, epsilon=1e-6):\n",
    "        mean = K.mean(inputs, axis=[-1], keepdims=True)\n",
    "        variance = K.var(inputs, axis=[-1], keepdims=True)\n",
    "        normalized_inputs = (inputs - mean) / (K.sqrt(variance) + epsilon)\n",
    "        return normalized_inputs * self.scale + self.shift\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreLayerNormPostResidualConnectionWrapper():\n",
    "    def __init__(self, layer, dropout_rate, *args, **kwargs):\n",
    "        self.layer = layer\n",
    "        self.layer_norm = LayerNormalization()\n",
    "        self.dropout_layer = Dropout(dropout_rate)\n",
    "        \n",
    "    def __call__(self, inputs, *args, **kwargs):\n",
    "        x = self.layer_norm(inputs)\n",
    "        x = self.layer(x)\n",
    "        output = self.dropout_layer(x)\n",
    "        return inputs + output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddPositionalEncoding(Layer):\n",
    "    def call(self, inputs):\n",
    "        data_type = inputs.dtype\n",
    "        print(\"add pos inputs\", inputs)\n",
    "        batch_size, max_len, emb_dim = tf.unstack(tf.shape(inputs))\n",
    "        # i is from 0 to 255 when emb_dim is 512\n",
    "        #so the doubled_i is from 0 to 510\n",
    "        doubled_i = K.arange(emb_dim) // 2 * 2\n",
    "        exponent = K.tile(K.expand_dims(doubled_i, 0), [max_len, 1])\n",
    "        denominator_matrix = K.pow(10000.0, K.cast(exponent / emb_dim, data_type))\n",
    "        \n",
    "        # since cos(x) = sin(x + Ï€/2), we convert the series of [sin, cos, sin, cos, ...]\n",
    "        # into [sin, sin, sin, sin, ...]\n",
    "        to_convert = K.cast(K.arange(emb_dim) % 2, data_type) * math.pi / 2\n",
    "        convert_matrix = K.tile(tf.expand_dims(to_convert, 0), [max_len, 1])\n",
    "        \n",
    "        seq_pos = K.arange(max_len)\n",
    "        numerator_matrix = K.cast(K.tile(K.expand_dims(seq_pos, 1), [1, emb_dim]), data_type)\n",
    "        \n",
    "        positinal_encoding = K.sin(numerator_matrix / denominator_matrix + convert_matrix)\n",
    "        batched_positional_encoding = K.tile(K.expand_dims(positinal_encoding, 0), [batch_size, 1, 1])\n",
    "        return inputs + batched_positional_encoding\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        print(\"input_shape\", input_shape)\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_ID = 0\n",
    "\n",
    "class TokenEmbedding(Layer):\n",
    "    def __init__(self, seq_len, vocab_size, emb_dim, data_type=\"float32\", *args, **kwargs):\n",
    "        self.seq_len = seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.emb_dim = emb_dim\n",
    "        self.data_type = data_type\n",
    "        super(TokenEmbedding, self).__init__(*args, **kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.embedding_layer = Embedding(self.vocab_size,\n",
    "                                   self.emb_dim,\n",
    "                                   embeddings_initializer=RandomNormal(mean=0.0, stddev=self.emb_dim**-0.5)\n",
    "                                  )\n",
    "        super(TokenEmbedding, self).build(input_shape)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        mask_for_pads = tf.to_float(tf.not_equal(inputs, PAD_ID))\n",
    "        embedding = self.embedding_layer(inputs)\n",
    "        pads_masked_embedding = embedding * tf.expand_dims(mask_for_pads, -1)\n",
    "        return pads_masked_embedding * (self.emb_dim ** 0.5)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0], self.seq_len, self.emb_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder():\n",
    "    def __init__(self, vocab_size, stack_num, head_num, emb_dim, dropout_rate, max_len, *args, **kwargs):\n",
    "        self.stack_num = stack_num\n",
    "        self.head_num = head_num\n",
    "        self.emb_dim = emb_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.token_emb_layer = TokenEmbedding(max_len, vocab_size, emb_dim)\n",
    "        self.add_pos_enc_layer = AddPositionalEncoding()\n",
    "        self.input_dropout_layer = Dropout(dropout_rate)\n",
    "        \n",
    "        self.attention_block_list = []\n",
    "        for _ in range(stack_num):\n",
    "            self_attention_layer = SelfAttention(emb_dim, head_num, dropout_rate, name=\"self_attention_layer\")\n",
    "            pffn_layer = PositionwiseFeedForwardNetwork(emb_dim, dropout_rate, \"pffn_layer\")\n",
    "            self.attention_block_list.append([\n",
    "                PreLayerNormPostResidualConnectionWrapper(self_attention_layer, dropout_rate, name=\"prepos_self_attention_wrapper\"),\n",
    "                PreLayerNormPostResidualConnectionWrapper(pffn_layer, dropout_rate, name=\"prepos_pffn_wrapper\")\n",
    "            ])\n",
    "        self.output_layer_norm = LayerNormalization()\n",
    "        \n",
    "    def __call__(self, inputs):\n",
    "        x = self.token_emb_layer(inputs)\n",
    "        print(\"first x\", x)\n",
    "        x = self.add_pos_enc_layer(x)\n",
    "        print(\"second x\", x)\n",
    "        x = self.input_dropout_layer(x)\n",
    "        print(\"third x\", x)\n",
    "        \n",
    "#        for i, set_of_layers_list in enumerate(self.attention_block_list):\n",
    "#            self_attention_layer, pffn_layer = tuple(set_of_layers_list)\n",
    "#            print(\"self_attention_layer\", self_attention_layer)\n",
    "#            print(\"fourth x\", x)\n",
    "#            x = self_attention_layer(x)\n",
    "#            print(\"fifth x\", x)\n",
    "#            x = pffn_layer(x)\n",
    "            \n",
    "        return self.output_layer_norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first x Tensor(\"token_embedding_5/mul_1:0\", shape=(?, 717, 512), dtype=float32)\n",
      "add pos inputs Tensor(\"token_embedding_5/mul_1:0\", shape=(?, 717, 512), dtype=float32)\n",
      "input_shape (None, 717, 512)\n",
      "second x Tensor(\"add_positional_encoding_5/add_1:0\", shape=(?, 717, 512), dtype=float32)\n",
      "third x Tensor(\"dropout_53/cond/Merge:0\", shape=(?, 717, 512), dtype=float32)\n",
      "encoder output Tensor(\"layer_normalization_65/add_1:0\", shape=(?, 717, 512), dtype=float32)\n",
      "outputs Tensor(\"dense_6/Softmax:0\", shape=(?, 717), dtype=float32)\n",
      "<__main__.LayerNormalization object at 0x7f1aaf666ba8>\n",
      "<keras.layers.core.Dropout object at 0x7f1aaf697f60>\n",
      "<__main__.AddPositionalEncoding object at 0x7f1aaf697ef0>\n",
      "<__main__.TokenEmbedding object at 0x7f1aaf697ac8>\n",
      "<keras.engine.input_layer.InputLayer object at 0x7f1aaf697d30>\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_6 (InputLayer)         (None, 717)               0         \n",
      "_________________________________________________________________\n",
      "token_embedding_5 (TokenEmbe (None, 717, 512)          0         \n",
      "_________________________________________________________________\n",
      "add_positional_encoding_5 (A (None, 717, 512)          0         \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 717, 512)          0         \n",
      "_________________________________________________________________\n",
      "layer_normalization_65 (Laye (None, 717, 512)          1024      \n",
      "=================================================================\n",
      "Total params: 1,024\n",
      "Trainable params: 1,024\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Transformer classification model\n",
    "MAX_LEN = 717\n",
    "\n",
    "inputs = Input(shape=(MAX_LEN,))\n",
    "transformer_encoder = Encoder(vocab_size=8000, stack_num=6, head_num=8, emb_dim=512, dropout_rate=0.1, max_len=MAX_LEN)\n",
    "encoder_output = transformer_encoder(inputs)\n",
    "print(\"encoder output\", encoder_output)\n",
    "summarized_vecs = encoder_output[:, 0, :]\n",
    "outputs = Dense(MAX_LEN, activation=\"softmax\")(summarized_vecs)\n",
    "print(\"outputs\", outputs)\n",
    "#model = Model(inputs, outputs)\n",
    "model = Model(inputs, encoder_output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
