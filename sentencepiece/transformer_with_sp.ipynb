{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I referred to the following webpages for the implementation.\n",
    "- Implementation of Transformer<br>\n",
    "https://qiita.com/halhorn/items/c91497522be27bde17ce<br>\n",
    "https://github.com/kpot/keras-transformer/tree/master/keras_transformer<br>\n",
    "https://github.com/Lsdefine/attention-is-all-you-need-keras<br>\n",
    "- Usage of \"\\_\\_call\\_\\_\" method<br>\n",
    "https://qiita.com/kyo-bad/items/439d8cc3a0424c45214a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Activation, Layer, Embedding, Input, Reshape, Lambda, Add\n",
    "from keras import backend as K\n",
    "from keras.initializers import RandomNormal\n",
    "from keras.utils import plot_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 512\n",
    "MAX_LEN = 717\n",
    "class_num = 9\n",
    "PAD_ID = 0\n",
    "warmup_steps = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention():\n",
    "    ## hidden_dim has to be multiples of head_num\n",
    "    def __init__(self, max_len, hidden_dim=512, head_num=8, dropout_rate=0.1, *args, **kwargs):\n",
    "        self.max_len = max_len\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.head_num = head_num\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.q_dense_layer = Dense(hidden_dim, use_bias=False)\n",
    "        self.k_dense_layer = Dense(hidden_dim, use_bias=False)\n",
    "        self.v_dense_layer = Dense(hidden_dim, use_bias=False)\n",
    "        self.output_dense_layer = Dense(hidden_dim, use_bias=False)\n",
    "        self.attention_dropout_layer = Dropout(dropout_rate)\n",
    "        \n",
    "    def split_heads(self, x):\n",
    "        def reshape(x):\n",
    "            x = tf.reshape(x, [-1, self.max_len, self.head_num, self.hidden_dim // self.head_num])\n",
    "            return tf.transpose(x, [0, 2, 1, 3])\n",
    "        \n",
    "        out = Lambda(reshape)(x)\n",
    "        return out\n",
    "    \n",
    "    def combine_heads(self, heads):\n",
    "        def reshape(x):\n",
    "            heads = tf.transpose(x, [0, 2, 1, 3])\n",
    "            return tf.reshape(x, [-1, self.max_len, self.hidden_dim])\n",
    "        \n",
    "        out = Lambda(reshape)(heads)\n",
    "        return out\n",
    "        \n",
    "    def __call__(self, query, memory):\n",
    "        #two arguments of query and memory are already encoded as embedded vectors for all words\n",
    "        q = self.q_dense_layer(query)\n",
    "        k = self.k_dense_layer(memory)\n",
    "        v = self.v_dense_layer(memory)\n",
    "        \n",
    "        q = self.split_heads(q)\n",
    "        k = self.split_heads(k)\n",
    "        v = self.split_heads(v)\n",
    "        \n",
    "        #for scaled dot-product\n",
    "        depth_inside_each_head = self.hidden_dim // self.head_num\n",
    "        q = Lambda(lambda x: x * (depth_inside_each_head ** -0.5))(q)\n",
    "        \n",
    "        #q.shape = (batch_size, head_num, query_len, emb_dim)\n",
    "        #k.shape = (batch_size, head_num, memory_len, emb_dim)\n",
    "        #batch_dot(q, k).shape = (batch_size, head_num, query_len, memory_len)\n",
    "        score = Lambda(lambda x: K.batch_dot(x[0], x[1], axes=[3, 3]))([q, k])\n",
    "        normalized_score = Activation(\"softmax\")(score)\n",
    "        normalized_score = self.attention_dropout_layer(normalized_score)\n",
    "        \n",
    "        #normalized_score.shape = (batch_size, head_num, query_length, memory_length)\n",
    "        #v.shape = (batch_size, head_num, memory_length, depth)\n",
    "        #attention_weighted_output.shape = (batch_size, head_num, query_length, depth)\n",
    "        attention_weighted_output = Lambda(lambda x: K.batch_dot(x[0], x[1], axes=[3, 2]))([normalized_score, v])\n",
    "        attention_weighted_output = self.combine_heads(attention_weighted_output)\n",
    "        return self.output_dense_layer(attention_weighted_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SlefAttention class inherits MultiheadAttention class so that it can make query and memory come from the same source.\n",
    "class SelfAttention(MultiheadAttention):\n",
    "    \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "    def __call__(self, query):\n",
    "        return super().__call__(query, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForwardNetwork():\n",
    "    \n",
    "    def __init__(self, hidden_dim, dropout_rate, *args, **kwargs):\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.first_dense_layer = Dense(hidden_dim*4, use_bias=True, activation=\"relu\")\n",
    "        self.second_dense_layer = Dense(hidden_dim, use_bias=True, activation=\"linear\")\n",
    "        self.dropout_layer = Dropout(dropout_rate)\n",
    "        \n",
    "    def __call__(self, inputs):\n",
    "        # make the network more flexible to learn for the first dense layer(non-linear transformation is used),\n",
    "        # and put the network back into the same hidden dim as original(linear transformation is used)\n",
    "        x = self.first_dense_layer(inputs)\n",
    "        x = self.dropout_layer(x)\n",
    "        return self.second_dense_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(Layer):\n",
    "    def __init__(self, axis=-1, **kwargs):\n",
    "        self.axis = axis\n",
    "        super(LayerNormalization, self).__init__(**kwargs)\n",
    "        \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config[\"axis\"] = self.axis\n",
    "        return config\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        hidden_dim = input_shape[-1]\n",
    "        self.scale = self.add_weight(\"layer_norm_scale\", shape=[hidden_dim],\n",
    "                                    initializer=\"ones\")\n",
    "        self.shift = self.add_weight(\"layer_norm_shift\", shape=[hidden_dim],\n",
    "                                    initializer=\"zeros\")\n",
    "        super(LayerNormalization, self).build(input_shape)\n",
    "        \n",
    "    def call(self, inputs, epsilon=1e-6):\n",
    "        mean = K.mean(inputs, axis=[-1], keepdims=True)\n",
    "        variance = K.var(inputs, axis=[-1], keepdims=True)\n",
    "        normalized_inputs = (inputs - mean) / (K.sqrt(variance) + epsilon)\n",
    "        return normalized_inputs * self.scale + self.shift\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreLayerNormPostResidualConnectionWrapper():\n",
    "    def __init__(self, layer, dropout_rate, *args, **kwargs):\n",
    "        self.layer = layer\n",
    "        self.layer_norm = LayerNormalization()\n",
    "        self.dropout_layer = Dropout(dropout_rate)\n",
    "        \n",
    "    def __call__(self, inputs, *args, **kwargs):\n",
    "        x = self.layer_norm(inputs)\n",
    "        x = self.layer(x)\n",
    "        outputs = self.dropout_layer(x)\n",
    "        results = Add()([inputs, outputs])\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddPositionalEncoding(Layer): \n",
    "    def call(self, inputs):\n",
    "        data_type = inputs.dtype\n",
    "        batch_size, max_len, emb_dim = tf.unstack(tf.shape(inputs))\n",
    "        # i is from 0 to 255 when emb_dim is 512\n",
    "        #so the doubled_i is from 0 to 510\n",
    "        doubled_i = K.arange(emb_dim) // 2 * 2\n",
    "        exponent = K.tile(K.expand_dims(doubled_i, 0), [max_len, 1])\n",
    "        denominator_matrix = K.pow(10000.0, K.cast(exponent / emb_dim, data_type))\n",
    "        \n",
    "        # since cos(x) = sin(x + Ï€/2), we convert the series of [sin, cos, sin, cos, ...]\n",
    "        # into [sin, sin, sin, sin, ...]\n",
    "        to_convert = K.cast(K.arange(emb_dim) % 2, data_type) * math.pi / 2\n",
    "        convert_matrix = K.tile(tf.expand_dims(to_convert, 0), [max_len, 1])\n",
    "        \n",
    "        seq_pos = K.arange(max_len)\n",
    "        numerator_matrix = K.cast(K.tile(K.expand_dims(seq_pos, 1), [1, emb_dim]), data_type)\n",
    "        \n",
    "        positinal_encoding = K.sin(numerator_matrix / denominator_matrix + convert_matrix)\n",
    "        batched_positional_encoding = K.tile(K.expand_dims(positinal_encoding, 0), [batch_size, 1, 1])\n",
    "        return inputs + batched_positional_encoding\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MakeZeroPads(Layer):\n",
    "    def __init__(self, seq_len, vocab_size, emb_dim, data_type=\"float32\", *args, **kwargs):\n",
    "        self.emb_dim = emb_dim\n",
    "        super(MakeZeroPads, self).__init__(*args, **kwargs)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        mask_for_pads = tf.to_float(tf.not_equal(inputs, PAD_ID))\n",
    "        pads_masked_embedding = inputs * mask_for_pads\n",
    "        return pads_masked_embedding * (self.emb_dim ** 0.5)\n",
    "    \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder():\n",
    "    def __init__(self, vocab_size, max_len, stack_num, head_num, emb_dim, dropout_rate, *args, **kwargs):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_len = max_len\n",
    "        self.stack_num = stack_num\n",
    "        self.head_num = head_num\n",
    "        self.emb_dim = emb_dim\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "        self.embedding_layer = Embedding(self.vocab_size,\n",
    "                           self.emb_dim,\n",
    "                           embeddings_initializer=RandomNormal(mean=0.0, stddev=self.emb_dim**-0.5)\n",
    "                          )\n",
    "        self.make_zero_pads_layer = MakeZeroPads(self.max_len, vocab_size, emb_dim)\n",
    "        self.add_pos_enc_layer = AddPositionalEncoding()\n",
    "        self.input_dropout_layer = Dropout(dropout_rate)\n",
    "        \n",
    "        self.attention_block_list = []\n",
    "        for _ in range(stack_num):\n",
    "            self_attention_layer = SelfAttention(self.max_len, self.emb_dim, self.head_num, self.dropout_rate)\n",
    "            pffn_layer = PositionwiseFeedForwardNetwork(self.emb_dim, self.dropout_rate)\n",
    "            self.attention_block_list.append([\n",
    "                PreLayerNormPostResidualConnectionWrapper(self_attention_layer, dropout_rate),\n",
    "                PreLayerNormPostResidualConnectionWrapper(pffn_layer, dropout_rate)\n",
    "            ])\n",
    "        self.output_layer_norm = LayerNormalization()\n",
    "        \n",
    "    def __call__(self, inputs):\n",
    "        x = self.embedding_layer(inputs)\n",
    "        x = self.make_zero_pads_layer(x)\n",
    "        x = self.add_pos_enc_layer(x)\n",
    "        x = self.input_dropout_layer(x)\n",
    "        \n",
    "        for i, set_of_layers_list in enumerate(self.attention_block_list):\n",
    "            self_attention_layer, pffn_layer = tuple(set_of_layers_list)\n",
    "            x = self_attention_layer(x)\n",
    "            x = pffn_layer(x)\n",
    "            \n",
    "        return self.output_layer_norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 717)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 717, 512)     4096000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "make_zero_pads_1 (MakeZeroPads) (None, 717, 512)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "add_positional_encoding_1 (AddP (None, 717, 512)     0           make_zero_pads_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 717, 512)     0           add_positional_encoding_1[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, 717, 512)     1024        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 717, 512)     262144      layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 8, 717, 64)   0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 717, 512)     262144      layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)               (None, 8, 717, 64)   0           lambda_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)               (None, 8, 717, 64)   0           dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)               (None, 8, 717, 717)  0           lambda_4[0][0]                   \n",
      "                                                                 lambda_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 8, 717, 717)  0           lambda_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 717, 512)     262144      layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 8, 717, 717)  0           activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)               (None, 8, 717, 64)   0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)               (None, 8, 717, 64)   0           dropout_2[0][0]                  \n",
      "                                                                 lambda_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)               (None, 717, 512)     0           lambda_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 717, 512)     262144      lambda_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 717, 512)     0           dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 717, 512)     0           dropout_1[0][0]                  \n",
      "                                                                 dropout_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_2 (LayerNor (None, 717, 512)     1024        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 717, 2048)    1050624     layer_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 717, 2048)    0           dense_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 717, 512)     1049088     dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 717, 512)     0           dense_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 717, 512)     0           add_1[0][0]                      \n",
      "                                                                 dropout_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 717, 512)     1024        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 717, 512)     262144      layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)               (None, 8, 717, 64)   0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 717, 512)     262144      layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_11 (Lambda)              (None, 8, 717, 64)   0           lambda_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_9 (Lambda)               (None, 8, 717, 64)   0           dense_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_12 (Lambda)              (None, 8, 717, 717)  0           lambda_11[0][0]                  \n",
      "                                                                 lambda_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 8, 717, 717)  0           lambda_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 717, 512)     262144      layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 8, 717, 717)  0           activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_10 (Lambda)              (None, 8, 717, 64)   0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lambda_13 (Lambda)              (None, 8, 717, 64)   0           dropout_6[0][0]                  \n",
      "                                                                 lambda_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_14 (Lambda)              (None, 717, 512)     0           lambda_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 717, 512)     262144      lambda_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 717, 512)     0           dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 717, 512)     0           add_2[0][0]                      \n",
      "                                                                 dropout_8[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_4 (LayerNor (None, 717, 512)     1024        add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 717, 2048)    1050624     layer_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 717, 2048)    0           dense_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 717, 512)     1049088     dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 717, 512)     0           dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 717, 512)     0           add_3[0][0]                      \n",
      "                                                                 dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_5 (LayerNor (None, 717, 512)     1024        add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 717, 512)     262144      layer_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_15 (Lambda)              (None, 8, 717, 64)   0           dense_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 717, 512)     262144      layer_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_18 (Lambda)              (None, 8, 717, 64)   0           lambda_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_16 (Lambda)              (None, 8, 717, 64)   0           dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_19 (Lambda)              (None, 8, 717, 717)  0           lambda_18[0][0]                  \n",
      "                                                                 lambda_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 8, 717, 717)  0           lambda_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 717, 512)     262144      layer_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 8, 717, 717)  0           activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_17 (Lambda)              (None, 8, 717, 64)   0           dense_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_20 (Lambda)              (None, 8, 717, 64)   0           dropout_10[0][0]                 \n",
      "                                                                 lambda_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_21 (Lambda)              (None, 717, 512)     0           lambda_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 717, 512)     262144      lambda_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)            (None, 717, 512)     0           dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 717, 512)     0           add_4[0][0]                      \n",
      "                                                                 dropout_12[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_6 (LayerNor (None, 717, 512)     1024        add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 717, 2048)    1050624     layer_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)            (None, 717, 2048)    0           dense_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 717, 512)     1049088     dropout_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_13 (Dropout)            (None, 717, 512)     0           dense_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 717, 512)     0           add_5[0][0]                      \n",
      "                                                                 dropout_13[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_7 (LayerNor (None, 717, 512)     1024        add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_19 (Dense)                (None, 717, 512)     262144      layer_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_22 (Lambda)              (None, 8, 717, 64)   0           dense_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 717, 512)     262144      layer_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_25 (Lambda)              (None, 8, 717, 64)   0           lambda_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_23 (Lambda)              (None, 8, 717, 64)   0           dense_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_26 (Lambda)              (None, 8, 717, 717)  0           lambda_25[0][0]                  \n",
      "                                                                 lambda_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 8, 717, 717)  0           lambda_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 717, 512)     262144      layer_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_14 (Dropout)            (None, 8, 717, 717)  0           activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_24 (Lambda)              (None, 8, 717, 64)   0           dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_27 (Lambda)              (None, 8, 717, 64)   0           dropout_14[0][0]                 \n",
      "                                                                 lambda_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_28 (Lambda)              (None, 717, 512)     0           lambda_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 717, 512)     262144      lambda_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_16 (Dropout)            (None, 717, 512)     0           dense_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 717, 512)     0           add_6[0][0]                      \n",
      "                                                                 dropout_16[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_8 (LayerNor (None, 717, 512)     1024        add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_23 (Dense)                (None, 717, 2048)    1050624     layer_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_15 (Dropout)            (None, 717, 2048)    0           dense_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_24 (Dense)                (None, 717, 512)     1049088     dropout_15[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_17 (Dropout)            (None, 717, 512)     0           dense_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 717, 512)     0           add_7[0][0]                      \n",
      "                                                                 dropout_17[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_9 (LayerNor (None, 717, 512)     1024        add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_25 (Dense)                (None, 717, 512)     262144      layer_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_29 (Lambda)              (None, 8, 717, 64)   0           dense_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_26 (Dense)                (None, 717, 512)     262144      layer_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "lambda_32 (Lambda)              (None, 8, 717, 64)   0           lambda_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_30 (Lambda)              (None, 8, 717, 64)   0           dense_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_33 (Lambda)              (None, 8, 717, 717)  0           lambda_32[0][0]                  \n",
      "                                                                 lambda_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 8, 717, 717)  0           lambda_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_27 (Dense)                (None, 717, 512)     262144      layer_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_18 (Dropout)            (None, 8, 717, 717)  0           activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_31 (Lambda)              (None, 8, 717, 64)   0           dense_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_34 (Lambda)              (None, 8, 717, 64)   0           dropout_18[0][0]                 \n",
      "                                                                 lambda_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_35 (Lambda)              (None, 717, 512)     0           lambda_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_28 (Dense)                (None, 717, 512)     262144      lambda_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 717, 512)     0           dense_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 717, 512)     0           add_8[0][0]                      \n",
      "                                                                 dropout_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_10 (LayerNo (None, 717, 512)     1024        add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_29 (Dense)                (None, 717, 2048)    1050624     layer_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_19 (Dropout)            (None, 717, 2048)    0           dense_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_30 (Dense)                (None, 717, 512)     1049088     dropout_19[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_21 (Dropout)            (None, 717, 512)     0           dense_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 717, 512)     0           add_9[0][0]                      \n",
      "                                                                 dropout_21[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_11 (LayerNo (None, 717, 512)     1024        add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_31 (Dense)                (None, 717, 512)     262144      layer_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_36 (Lambda)              (None, 8, 717, 64)   0           dense_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_32 (Dense)                (None, 717, 512)     262144      layer_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_39 (Lambda)              (None, 8, 717, 64)   0           lambda_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_37 (Lambda)              (None, 8, 717, 64)   0           dense_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_40 (Lambda)              (None, 8, 717, 717)  0           lambda_39[0][0]                  \n",
      "                                                                 lambda_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 8, 717, 717)  0           lambda_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_33 (Dense)                (None, 717, 512)     262144      layer_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_22 (Dropout)            (None, 8, 717, 717)  0           activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "lambda_38 (Lambda)              (None, 8, 717, 64)   0           dense_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_41 (Lambda)              (None, 8, 717, 64)   0           dropout_22[0][0]                 \n",
      "                                                                 lambda_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "lambda_42 (Lambda)              (None, 717, 512)     0           lambda_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_34 (Dense)                (None, 717, 512)     262144      lambda_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_24 (Dropout)            (None, 717, 512)     0           dense_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 717, 512)     0           add_10[0][0]                     \n",
      "                                                                 dropout_24[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_12 (LayerNo (None, 717, 512)     1024        add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_35 (Dense)                (None, 717, 2048)    1050624     layer_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_23 (Dropout)            (None, 717, 2048)    0           dense_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dense_36 (Dense)                (None, 717, 512)     1049088     dropout_23[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_25 (Dropout)            (None, 717, 512)     0           dense_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 717, 512)     0           add_11[0][0]                     \n",
      "                                                                 dropout_25[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_13 (LayerNo (None, 717, 512)     1024        add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lambda_43 (Lambda)              (None, 512)          0           layer_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_37 (Dense)                (None, 9)            4617        lambda_43[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 23,003,657\n",
      "Trainable params: 23,003,657\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Transformer classification model\n",
    "inputs = Input(shape=(MAX_LEN,))\n",
    "transformer_encoder = Encoder(vocab_size=8000, stack_num=6, head_num=8, emb_dim=512, dropout_rate=0.1, max_len=MAX_LEN)\n",
    "encoder_output = transformer_encoder(inputs)\n",
    "#Since the task is text classification, we just need the first token for each sample\n",
    "summarized_vecs = Lambda(lambda x: x[:, 0, :])(encoder_output)\n",
    "outputs = Dense(class_num, activation=\"softmax\")(summarized_vecs)\n",
    "model = Model(inputs, outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(model, to_file=\"transformer_encoder.png\", show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning rate for Adam\n",
    "class LRSchedulerPerStep(Callback):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        self.d_model = d_model\n",
    "        self.warmup_step = warmup_steps\n",
    "        self.step_num = 0\n",
    "        \n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        self.step_num += 1\n",
    "        updated_lr = self.d_model * min(self.step_num ** (-0.5), self.step_num * (self.warmup_step ** (-1.5)))\n",
    "        K.set_value(self.model.optimizer.lr, updated_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_schedular = LRSchedulerPerStep(d_model, warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Adam(beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About customized loss function\n",
    "https://stackoverflow.com/questions/50063613/add-loss-function-in-keras\n",
    "https://github.com/kpot/keras-transformer/blob/b9d4e76c535c0c62cadc73e37416e4dc18b635ca/keras_transformer/bert.py#L212"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=opt, loss=label_smoothed_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([x_train, y_train], batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test), callbacks=[lr_scheduler])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
