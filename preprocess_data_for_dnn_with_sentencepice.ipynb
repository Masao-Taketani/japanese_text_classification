{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from random import shuffle\n",
    "import random\n",
    "\n",
    "def pre_process_data(filepath):\n",
    "    dataset = []\n",
    "    dirs = []\n",
    "    \n",
    "    for dir in os.listdir(filepath):\n",
    "        if os.path.isdir(filepath + dir):\n",
    "            dirs.append(filepath + dir)\n",
    "            \n",
    "    for i, dir_path in enumerate(dirs):\n",
    "        dir_name = dir_path.split('/')[-1]\n",
    "        label_id = i\n",
    "        print('label_id: {}, dir_name: {}'.format(label_id, dir_name))\n",
    "        \n",
    "        for filename in glob.glob(os.path.join(filepath, dir_name, dir_name + \"*.txt\")):\n",
    "            with open(filename, 'r' ,encoding=\"utf-8\") as f:\n",
    "                #datasets hold sets of tuples such as (label, input text)\n",
    "                dataset.append((label_id, f.read()))\n",
    "                \n",
    "    random.seed(1234)            \n",
    "    shuffle(dataset)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_id: 0, dir_name: dokujo-tsushin\n",
      "label_id: 1, dir_name: it-life-hack\n",
      "label_id: 2, dir_name: kaden-channel\n",
      "label_id: 3, dir_name: livedoor-homme\n",
      "label_id: 4, dir_name: movie-enter\n",
      "label_id: 5, dir_name: peachy\n",
      "label_id: 6, dir_name: smax\n",
      "label_id: 7, dir_name: sports-watch\n",
      "label_id: 8, dir_name: topic-news\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7,\n",
       " 'http://news.livedoor.com/article/detail/6283739/\\n2012-02-16T08:00:00+0900\\n「もう仮病使えよ」香川、長友ら招集に  \\n14日、日本サッカー協会は29日に行なわれるW杯アジア3次予選・ウズベキスタン戦に向けて香川真司(ドルトムント/ドイツ)、長友佑都(インテル/イタリア)ら海外組14人の招集を求め、所属クラブに協力を要請する文書を送付したと発表した。負傷中の本田圭佑(CSKAモスクワ/ロシア)は含まれないものの、所属クラブへの完全移籍が見送られた宇佐美貴史(バイエルン/ドイツ)、移籍したばかりの家長昭博(蔚山現代/韓国)らの名前もあるという。\\n\\n しかし、日本はすでに最終予選進出を決めており、この試合は形としては消化試合となる。海外組を含めたフルメンバーを投入する意図は「試合間隔をあけすぎないこと」などが予想されるが、特に海外で好調を維持する香川、長友の招集にはサッカーファンからさまざまな反応が出た。\\n\\n 「新戦力発掘しないでどうすんだよ」「宇佐美は五輪のほうに呼べよ」「ジーコ解任デモやった奴、出番だぞ」「もう仮病使えよ」「ドルトムントは招集文書破り捨ててOK」といった、海外組の招集に反対する声が高まった一方で、「ここで呼ばなきゃ、6月の最終予選にぶっつけ本番だぞ」「海外組がいるのといないのとじゃスポンサー料が全然違うからな」「いやこれくらいこなせるだろwお前ら過保護w」といった意見も散見された。\\n\\n■関連リンク\\n・香川真司の得点で勝利。チームも香川も好調を維持。\\u3000【ボルシア・ドルトムントｖｓレヴァークーゼン】\\n・伊紙、長友の必要性を力説し指揮官を酷評「早く起用すべきだった」\\n・【加部究コラム】プロの充実がなければ未来は暗い\\n')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"livedoor_data/text/\"\n",
    "\n",
    "dataset = pre_process_data(path)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7367"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', 'h', 't', 't', 'p', ':', '/', '/', 'ne', 'w', 's', '.', 'l', 'ive', 'd', 'o', 'or', '.', 'com', '/', 'art', 'ic', 'le', '/', 'de', 'ta', 'il', '/', '6', '28', '37', '39', '/', '▁2012', '-0', '2', '-', '16', 'T', '0', '8', ':', '00', ':', '00', '+', '0', '9', '00', '▁「', 'もう', '仮', '病', '使', 'え', 'よ', '」', '香', '川', '、', '長', '友', 'ら', '招', '集', 'に', '▁', '14', '日', '、', '日本', 'サッカー', '協会', 'は', '29', '日に', '行', 'な', 'われる', 'W', '杯', 'アジア', '3', '次', '予選', '・', 'ウ', 'ズ', 'ベ', 'キ', 'スタン', '戦', 'に向けて', '香', '川', '真', '司', '(', 'ドル', 'ト', 'ム', 'ント', '/', 'ドイツ', ')', '、', '長', '友', '佑', '都', '(', 'イン', 'テル', '/', 'イタリア', ')', 'ら', '海外', '組', '14', '人の', '招', '集', 'を', '求め', '、', '所属', 'クラブ', 'に', '協力', 'を', '要請', 'する', '文書', 'を', '送', '付', 'した', 'と', '発表した', '。', '負傷', '中の', '本', '田', '圭', '佑', '(', 'C', 'S', 'KA', 'モスクワ', '/', 'ロシア', ')', 'は', '含まれ', 'ない', 'ものの', '、', '所属', 'クラブ', 'への', '完全', '移籍', 'が', '見', '送', 'られた', '宇', '佐', '美', '貴', '史', '(', 'バイ', 'エル', 'ン', '/', 'ドイツ', ')', '、', '移籍', 'した', 'ばかり', 'の', '家', '長', '昭', '博', '(', '蔚', '山', '現代', '/', '韓国', ')', 'ら', 'の名前', 'もある', 'という', '。', '▁しかし', '、', '日本', 'は', 'すでに', '最終', '予選', '進出', 'を', '決め', 'ており', '、', 'この', '試合', 'は', '形', 'としては', '消', '化', '試合', 'となる', '。', '海外', '組', 'を含め', 'た', 'フル', 'メンバー', 'を', '投入', 'する', '意図', 'は', '「', '試合', '間', '隔', 'を', 'あ', 'け', 'すぎ', 'ない', 'こと', '」', 'などが', '予想', 'される', 'が', '、', '特に', '海外', 'で', '好', '調', 'を', '維持', 'する', '香', '川', '、', '長', '友', 'の', '招', '集', 'には', 'サッカー', 'ファン', 'から', 'さまざまな', '反応', 'が出', 'た', '。', '▁「', '新', '戦', '力', '発掘', 'しない', 'で', 'どう', 'す', 'んだ', 'よ', '」「', '宇', '佐', '美', 'は', '五', '輪', 'の', 'ほう', 'に', '呼', 'べ', 'よ', '」「', 'ジー', 'コ', '解', '任', 'デ', 'モ', 'や', 'った', '奴', '、', '出', '番', 'だ', 'ぞ', '」「', 'もう', '仮', '病', '使', 'え', 'よ', '」「', 'ドル', 'ト', 'ム', 'ント', 'は', '招', '集', '文書', '破', 'り', '捨て', 'て', 'O', 'K', '」', 'といった', '、', '海外', '組', 'の', '招', '集', 'に', '反対', 'する', '声', 'が', '高', 'まった', '一方で', '、「', 'ここで', '呼', 'ば', 'な', 'き', 'ゃ', '、', '6', '月', 'の', '最終', '予選', 'に', 'ぶ', 'っ', 'つけ', '本', '番', 'だ', 'ぞ', '」「', '海外', '組', 'がいる', 'の', 'と', 'いない', 'の', 'と', 'じゃ', 'スポンサー', '料', 'が', '全', '然', '違', 'う', 'から', 'な', '」「', 'い', 'や', 'これ', 'く', 'らい', 'こ', 'な', 'せる', 'だ', 'ろ', 'w', 'お', '前', 'ら', '過', '保護', 'w', '」', 'といった', '意見', 'も', '散', '見', 'された', '。', '▁', '■', '関連', 'リンク', '▁', '・', '香', '川', '真', '司', 'の', '得点', 'で', '勝利', '。', 'チーム', 'も', '香', '川', 'も', '好', '調', 'を', '維持', '。', '▁', '【', 'ボル', 'シア', '・', 'ドル', 'ト', 'ム', 'ント', 'v', 's', 'レ', 'ヴァー', 'クー', 'ゼン', '】', '▁', '・', '伊', '紙', '、', '長', '友', 'の', '必要', '性を', '力', '説', 'し', '指揮', '官', 'を', '酷', '評', '「', '早く', '起用', 'すべき', 'だった', '」', '▁', '・', '【', '加', '部', '究', 'コ', 'ラム', '】', 'プロ', 'の', '充', '実', 'が', 'なければ', '未来', 'は', '暗', 'い']\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "tokenizer = spm.SentencePieceProcessor()\n",
    "tokenizer.Load(\"wiki_data/wikiextractor/spm.model\")\n",
    "\n",
    "print(tokenizer.EncodeAsPieces(dataset[0][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_with_sp(dataset):\n",
    "    tokenized_data = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        tokens_list_for_each_sample = tokenizer.EncodeAsPieces(sample[1])\n",
    "        tokenized_data.append(tokens_list_for_each_sample)\n",
    "        \n",
    "    return tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_labels(dataset):\n",
    "    labels = []\n",
    "    for sample in dataset:\n",
    "        labels.append(sample[0])\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_inputs = tokenize_with_sp(dataset)\n",
    "labels = collect_labels(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tokenized_inputs): 7367\n",
      "len(tokenized_inputs[0]): 503\n",
      "len(labels): 7367\n"
     ]
    }
   ],
   "source": [
    "print('len(tokenized_inputs):', len(tokenized_inputs))\n",
    "print('len(tokenized_inputs[0]):', len(tokenized_inputs[0]))\n",
    "print('len(labels):', len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data = int(len(tokenized_inputs)* 0.8)\n",
    "\n",
    "x_train = tokenized_inputs[:split_data]\n",
    "x_test = tokenized_inputs[split_data:]\n",
    "y_train= labels[:split_data]\n",
    "y_test = labels[split_data:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max step-length: 7313\n"
     ]
    }
   ],
   "source": [
    "#To check the maximum input steps among the entire dataset\n",
    "max = 0\n",
    "for elem in tokenized_inputs:\n",
    "    if len(elem) > max:\n",
    "        max = len(elem)\n",
    "        \n",
    "print('max step-length:', max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min step-length: 69\n"
     ]
    }
   ],
   "source": [
    "#To check the minimus input steps among the entire dataset\n",
    "min = 7313\n",
    "for elem in tokenized_inputs:\n",
    "    if len(elem) < min:\n",
    "        min = len(elem)\n",
    "        \n",
    "print('min step-length:', min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg step-length: 890.2934708836705\n"
     ]
    }
   ],
   "source": [
    "#To check the average input steps among the entire dataset\n",
    "sum = 0\n",
    "total_num = len(tokenized_inputs)\n",
    "for elem in tokenized_inputs:\n",
    "     sum += len(elem)\n",
    "        \n",
    "print('avg step-length:', sum/total_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 7313"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def pad_or_truncate_inputs(data, max_len):\n",
    "    new_data = []\n",
    "    pad_list = [\"PAD\"]\n",
    "        \n",
    "    for sample in tqdm(data):\n",
    "        if len(sample) >= max_len:\n",
    "            tmp = sample[:max_len]\n",
    "        else:\n",
    "            tmp = sample\n",
    "            num_of_pads_needed = max_len - len(sample)\n",
    "            for _ in range(num_of_pads_needed):\n",
    "                tmp.append(pad_list)\n",
    "                \n",
    "        new_data.append(tmp)\n",
    "        \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5893/5893 [00:00<00:00, 63324.24it/s]\n",
      "100%|██████████| 1474/1474 [00:00<00:00, 78837.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: 5893\n",
      "x_test.shape: 1474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train = pad_or_truncate_inputs(x_train, max_len)\n",
    "x_test = pad_or_truncate_inputs(x_test, max_len)\n",
    "\n",
    "print('len(x_train):', len(x_train))\n",
    "print('len(x_test):', len(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(x_train[0]): 512\n",
      "len(x_test[1]): 512\n",
      "len(x_test[2]): 512\n"
     ]
    }
   ],
   "source": [
    "print('len(x_train[0]):', len(x_train[0]))\n",
    "print('len(x_test[1]):', len(x_test[1]))\n",
    "print('len(x_test[2]):', len(x_test[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train.shape: (5893, 9)\n",
      "y_test.shape: (1474, 9)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "y_train = to_categorical(y_train.astype('int32'), 9)\n",
    "y_test = to_categorical(y_test.astype('int32'), 9)\n",
    "\n",
    "print('y_train.shape:', y_train.shape)\n",
    "print('y_test.shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def save_data_as_tfrecord(X, Y, tfrecord_filename):\n",
    "    with tf.python_io.TFRecordWriter(tfrecord_filename) as w:\n",
    "        for x, y in tqdm(zip(X, Y)):\n",
    "            x = x.reshape(-1)\n",
    "            features = tf.train.Features(feature = {\n",
    "                'X': tf.train.Feature(float_list = tf.train.FloatList(value=x)),\n",
    "                'Y': tf.train.Feature(float_list = tf.train.FloatList(value=y))\n",
    "            })\n",
    "            \n",
    "            example = tf.train.Example(features=features)\n",
    "            w.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5893it [51:02,  1.94it/s]\n",
      "1474it [12:44,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFRcord files are created for training and test data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "save_data_as_tfrecord(x_train, y_train, 'train.tfrecord')\n",
    "save_data_as_tfrecord(x_test, y_test, 'test.tfrecord')\n",
    "print('TFRcord files are created for training and test data.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
