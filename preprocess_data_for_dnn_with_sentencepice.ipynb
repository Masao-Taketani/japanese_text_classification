{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from random import shuffle\n",
    "import random\n",
    "\n",
    "def pre_process_data(filepath):\n",
    "    dataset = []\n",
    "    dirs = []\n",
    "    \n",
    "    for dir in os.listdir(filepath):\n",
    "        if os.path.isdir(filepath + dir):\n",
    "            dirs.append(filepath + dir)\n",
    "            \n",
    "    for i, dir_path in enumerate(dirs):\n",
    "        dir_name = dir_path.split('/')[-1]\n",
    "        label_id = i\n",
    "        print('label_id: {}, dir_name: {}'.format(label_id, dir_name))\n",
    "        \n",
    "        for filename in glob.glob(os.path.join(filepath, dir_name, dir_name + \"*.txt\")):\n",
    "            with open(filename, 'r' ,encoding=\"utf-8\") as f:\n",
    "                #datasets hold sets of tuples such as (label, input text)\n",
    "                dataset.append((label_id, f.read()))\n",
    "                \n",
    "    random.seed(1234)            \n",
    "    shuffle(dataset)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_id: 0, dir_name: dokujo-tsushin\n",
      "label_id: 1, dir_name: it-life-hack\n",
      "label_id: 2, dir_name: kaden-channel\n",
      "label_id: 3, dir_name: livedoor-homme\n",
      "label_id: 4, dir_name: movie-enter\n",
      "label_id: 5, dir_name: peachy\n",
      "label_id: 6, dir_name: smax\n",
      "label_id: 7, dir_name: sports-watch\n",
      "label_id: 8, dir_name: topic-news\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7,\n",
       " 'http://news.livedoor.com/article/detail/6283739/\\n2012-02-16T08:00:00+0900\\n「もう仮病使えよ」香川、長友ら招集に  \\n14日、日本サッカー協会は29日に行なわれるW杯アジア3次予選・ウズベキスタン戦に向けて香川真司(ドルトムント/ドイツ)、長友佑都(インテル/イタリア)ら海外組14人の招集を求め、所属クラブに協力を要請する文書を送付したと発表した。負傷中の本田圭佑(CSKAモスクワ/ロシア)は含まれないものの、所属クラブへの完全移籍が見送られた宇佐美貴史(バイエルン/ドイツ)、移籍したばかりの家長昭博(蔚山現代/韓国)らの名前もあるという。\\n\\n しかし、日本はすでに最終予選進出を決めており、この試合は形としては消化試合となる。海外組を含めたフルメンバーを投入する意図は「試合間隔をあけすぎないこと」などが予想されるが、特に海外で好調を維持する香川、長友の招集にはサッカーファンからさまざまな反応が出た。\\n\\n 「新戦力発掘しないでどうすんだよ」「宇佐美は五輪のほうに呼べよ」「ジーコ解任デモやった奴、出番だぞ」「もう仮病使えよ」「ドルトムントは招集文書破り捨ててOK」といった、海外組の招集に反対する声が高まった一方で、「ここで呼ばなきゃ、6月の最終予選にぶっつけ本番だぞ」「海外組がいるのといないのとじゃスポンサー料が全然違うからな」「いやこれくらいこなせるだろwお前ら過保護w」といった意見も散見された。\\n\\n■関連リンク\\n・香川真司の得点で勝利。チームも香川も好調を維持。\\u3000【ボルシア・ドルトムントｖｓレヴァークーゼン】\\n・伊紙、長友の必要性を力説し指揮官を酷評「早く起用すべきだった」\\n・【加部究コラム】プロの充実がなければ未来は暗い\\n')"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"livedoor_data/text/\"\n",
    "\n",
    "dataset = pre_process_data(path)\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7367"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', 'h', 't', 't', 'p', ':', '/', '/', 'ne', 'w', 's', '.', 'l', 'ive', 'd', 'o', 'or', '.', 'com', '/', 'art', 'ic', 'le', '/', 'de', 'ta', 'il', '/', '6', '28', '37', '39', '/', '▁2012', '-0', '2', '-', '16', 'T', '0', '8', ':', '00', ':', '00', '+', '0', '9', '00', '▁「', 'もう', '仮', '病', '使', 'え', 'よ', '」', '香', '川', '、', '長', '友', 'ら', '招', '集', 'に', '▁', '14', '日', '、', '日本', 'サッカー', '協会', 'は', '29', '日に', '行', 'な', 'われる', 'W', '杯', 'アジア', '3', '次', '予選', '・', 'ウ', 'ズ', 'ベ', 'キ', 'スタン', '戦', 'に向けて', '香', '川', '真', '司', '(', 'ドル', 'ト', 'ム', 'ント', '/', 'ドイツ', ')', '、', '長', '友', '佑', '都', '(', 'イン', 'テル', '/', 'イタリア', ')', 'ら', '海外', '組', '14', '人の', '招', '集', 'を', '求め', '、', '所属', 'クラブ', 'に', '協力', 'を', '要請', 'する', '文書', 'を', '送', '付', 'した', 'と', '発表した', '。', '負傷', '中の', '本', '田', '圭', '佑', '(', 'C', 'S', 'KA', 'モスクワ', '/', 'ロシア', ')', 'は', '含まれ', 'ない', 'ものの', '、', '所属', 'クラブ', 'への', '完全', '移籍', 'が', '見', '送', 'られた', '宇', '佐', '美', '貴', '史', '(', 'バイ', 'エル', 'ン', '/', 'ドイツ', ')', '、', '移籍', 'した', 'ばかり', 'の', '家', '長', '昭', '博', '(', '蔚', '山', '現代', '/', '韓国', ')', 'ら', 'の名前', 'もある', 'という', '。', '▁しかし', '、', '日本', 'は', 'すでに', '最終', '予選', '進出', 'を', '決め', 'ており', '、', 'この', '試合', 'は', '形', 'としては', '消', '化', '試合', 'となる', '。', '海外', '組', 'を含め', 'た', 'フル', 'メンバー', 'を', '投入', 'する', '意図', 'は', '「', '試合', '間', '隔', 'を', 'あ', 'け', 'すぎ', 'ない', 'こと', '」', 'などが', '予想', 'される', 'が', '、', '特に', '海外', 'で', '好', '調', 'を', '維持', 'する', '香', '川', '、', '長', '友', 'の', '招', '集', 'には', 'サッカー', 'ファン', 'から', 'さまざまな', '反応', 'が出', 'た', '。', '▁「', '新', '戦', '力', '発掘', 'しない', 'で', 'どう', 'す', 'んだ', 'よ', '」「', '宇', '佐', '美', 'は', '五', '輪', 'の', 'ほう', 'に', '呼', 'べ', 'よ', '」「', 'ジー', 'コ', '解', '任', 'デ', 'モ', 'や', 'った', '奴', '、', '出', '番', 'だ', 'ぞ', '」「', 'もう', '仮', '病', '使', 'え', 'よ', '」「', 'ドル', 'ト', 'ム', 'ント', 'は', '招', '集', '文書', '破', 'り', '捨て', 'て', 'O', 'K', '」', 'といった', '、', '海外', '組', 'の', '招', '集', 'に', '反対', 'する', '声', 'が', '高', 'まった', '一方で', '、「', 'ここで', '呼', 'ば', 'な', 'き', 'ゃ', '、', '6', '月', 'の', '最終', '予選', 'に', 'ぶ', 'っ', 'つけ', '本', '番', 'だ', 'ぞ', '」「', '海外', '組', 'がいる', 'の', 'と', 'いない', 'の', 'と', 'じゃ', 'スポンサー', '料', 'が', '全', '然', '違', 'う', 'から', 'な', '」「', 'い', 'や', 'これ', 'く', 'らい', 'こ', 'な', 'せる', 'だ', 'ろ', 'w', 'お', '前', 'ら', '過', '保護', 'w', '」', 'といった', '意見', 'も', '散', '見', 'された', '。', '▁', '■', '関連', 'リンク', '▁', '・', '香', '川', '真', '司', 'の', '得点', 'で', '勝利', '。', 'チーム', 'も', '香', '川', 'も', '好', '調', 'を', '維持', '。', '▁', '【', 'ボル', 'シア', '・', 'ドル', 'ト', 'ム', 'ント', 'v', 's', 'レ', 'ヴァー', 'クー', 'ゼン', '】', '▁', '・', '伊', '紙', '、', '長', '友', 'の', '必要', '性を', '力', '説', 'し', '指揮', '官', 'を', '酷', '評', '「', '早く', '起用', 'すべき', 'だった', '」', '▁', '・', '【', '加', '部', '究', 'コ', 'ラム', '】', 'プロ', 'の', '充', '実', 'が', 'なければ', '未来', 'は', '暗', 'い']\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "tokenizer = spm.SentencePieceProcessor()\n",
    "tokenizer.Load(\"wiki_data/wikiextractor/spm.model\")\n",
    "\n",
    "test = tokenizer.EncodeAsPieces(dataset[0][1])\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 522, 268, 268, 334, 163, 317, 317, 2670, 706, 160, 106, 486, 4491, 497, 192, 531, 106, 5232, 317, 4349, 751, 918, 317, 4085, 3281, 1918, 317, 59, 739, 2383, 2552, 317, 2300, 2528, 28, 82, 272, 179, 290, 62, 163, 545, 163, 545, 1915, 290, 65, 545, 1411, 2203, 1738, 1191, 590, 265, 899, 18, 1910, 98, 3, 84, 1293, 97, 1951, 473, 10, 6, 361, 30, 3, 91, 1570, 1177, 7, 759, 126, 167, 40, 2858, 345, 3181, 1779, 32, 357, 2943, 11, 132, 95, 358, 159, 1227, 99, 5443, 1910, 98, 593, 1029, 15, 1172, 44, 80, 1435, 317, 581, 12, 3, 84, 1293, 6552, 949, 15, 269, 1866, 317, 1122, 12, 97, 2743, 606, 361, 478, 1951, 473, 8, 1825, 3, 860, 1074, 10, 2139, 8, 4815, 25, 3257, 8, 789, 530, 17, 16, 4029, 5, 4306, 1426, 73, 74, 6140, 6552, 15, 145, 130, 5026, 4909, 317, 959, 12, 7, 2172, 207, 1153, 3, 860, 1074, 261, 2343, 2758, 9, 198, 789, 342, 2487, 671, 529, 2627, 861, 15, 994, 838, 131, 317, 581, 12, 3, 2758, 17, 4398, 4, 122, 84, 4682, 1987, 15, 7162, 63, 1941, 317, 1716, 12, 97, 3083, 711, 61, 5, 2559, 3, 91, 7, 3989, 1560, 2943, 2090, 8, 2692, 727, 3, 52, 569, 7, 284, 877, 1558, 164, 569, 223, 5, 2743, 606, 3229, 31, 1687, 1569, 8, 4564, 25, 5476, 7, 20, 569, 329, 3538, 8, 293, 420, 4196, 207, 332, 18, 1405, 5128, 101, 9, 3, 748, 2743, 14, 845, 618, 8, 2699, 25, 1910, 98, 3, 84, 1293, 4, 1951, 473, 53, 1570, 1207, 23, 4572, 2566, 2024, 31, 5, 1411, 154, 99, 232, 5716, 1269, 14, 1600, 156, 540, 899, 767, 2487, 671, 529, 7, 810, 1216, 4, 3220, 10, 2076, 791, 899, 767, 1487, 93, 737, 421, 239, 247, 29, 105, 6000, 3, 111, 430, 406, 3371, 767, 2203, 1738, 1191, 590, 265, 899, 767, 1172, 44, 80, 1435, 7, 1951, 473, 3257, 776, 46, 5324, 66, 376, 364, 18, 1021, 3, 2743, 606, 4, 1951, 473, 10, 1956, 25, 1103, 9, 148, 1650, 2096, 250, 2653, 2076, 407, 40, 89, 5896, 3, 59, 19, 4, 1560, 2943, 10, 580, 360, 2228, 73, 430, 406, 3371, 767, 2743, 606, 3008, 4, 16, 4331, 4, 16, 4566, 5604, 1004, 9, 236, 2244, 2796, 133, 23, 40, 767, 41, 29, 785, 54, 2954, 331, 40, 2778, 406, 841, 706, 267, 134, 97, 1595, 1991, 706, 18, 1021, 3439, 24, 1865, 198, 35, 5, 6, 6916, 1540, 4618, 6, 11, 1910, 98, 593, 1029, 4, 2100, 14, 1085, 5, 433, 24, 1910, 98, 24, 845, 618, 8, 2699, 5, 6, 6050, 2039, 1460, 11, 1172, 44, 80, 1435, 1078, 160, 120, 2523, 2920, 2919, 6051, 6, 11, 1105, 1285, 3, 84, 1293, 4, 1617, 2504, 232, 677, 21, 1324, 378, 8, 5642, 1165, 20, 4210, 3496, 4264, 278, 18, 6, 11, 6050, 740, 94, 6175, 93, 1773, 6051, 524, 4, 2722, 291, 9, 3764, 5134, 7, 2848, 41]\n"
     ]
    }
   ],
   "source": [
    "test_list = []\n",
    "for token in test:\n",
    "    test_list.append(tokenizer.piece_to_id(token))\n",
    "\n",
    "print(test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_encode_with_sp(dataset):\n",
    "    tokenized_and_encoded_data = []\n",
    "    \n",
    "    for sample in dataset:\n",
    "        tokens_list_for_each_sample = tokenizer.EncodeAsPieces(sample[1])\n",
    "        encoded_list = []\n",
    "        for token in tokens_list_for_each_sample:\n",
    "            encoded_list.append(tokenizer.piece_to_id(token))\n",
    "        tokenized_and_encoded_data.append(encoded_list)\n",
    "        \n",
    "    return tokenized_and_encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_labels(dataset):\n",
    "    labels = []\n",
    "    for sample in dataset:\n",
    "        labels.append(sample[0])\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_and_encoded_inputs = tokenize_and_encode_with_sp(dataset)\n",
    "labels = collect_labels(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tokenized_and_encoded_inputs): 7367\n",
      "len(tokenized_and_encoded_inputs[0]): 503\n",
      "len(labels): 7367\n"
     ]
    }
   ],
   "source": [
    "print('len(tokenized_and_encoded_inputs):', len(tokenized_and_encoded_inputs))\n",
    "print('len(tokenized_and_encoded_inputs[0]):', len(tokenized_and_encoded_inputs[0]))\n",
    "print('len(labels):', len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data = int(len(tokenized_and_encoded_inputs)* 0.8)\n",
    "\n",
    "x_train = tokenized_and_encoded_inputs[:split_data]\n",
    "x_test = tokenized_and_encoded_inputs[split_data:]\n",
    "y_train= labels[:split_data]\n",
    "y_test = labels[split_data:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6, 522, 268, 268, 334, 163, 317, 317, 2670, 706, 160, 106, 486, 4491, 497, 192, 531, 106, 5232, 317, 4349, 751, 918, 317, 4085, 3281, 1918, 317, 59, 739, 2383, 2552, 317, 2300, 2528, 28, 82, 272, 179, 290, 62, 163, 545, 163, 545, 1915, 290, 65, 545, 1411, 2203, 1738, 1191, 590, 265, 899, 18, 1910, 98, 3, 84, 1293, 97, 1951, 473, 10, 6, 361, 30, 3, 91, 1570, 1177, 7, 759, 126, 167, 40, 2858, 345, 3181, 1779, 32, 357, 2943, 11, 132, 95, 358, 159, 1227, 99, 5443, 1910, 98, 593, 1029, 15, 1172, 44, 80, 1435, 317, 581, 12, 3, 84, 1293, 6552, 949, 15, 269, 1866, 317, 1122, 12, 97, 2743, 606, 361, 478, 1951, 473, 8, 1825, 3, 860, 1074, 10, 2139, 8, 4815, 25, 3257, 8, 789, 530, 17, 16, 4029, 5, 4306, 1426, 73, 74, 6140, 6552, 15, 145, 130, 5026, 4909, 317, 959, 12, 7, 2172, 207, 1153, 3, 860, 1074, 261, 2343, 2758, 9, 198, 789, 342, 2487, 671, 529, 2627, 861, 15, 994, 838, 131, 317, 581, 12, 3, 2758, 17, 4398, 4, 122, 84, 4682, 1987, 15, 7162, 63, 1941, 317, 1716, 12, 97, 3083, 711, 61, 5, 2559, 3, 91, 7, 3989, 1560, 2943, 2090, 8, 2692, 727, 3, 52, 569, 7, 284, 877, 1558, 164, 569, 223, 5, 2743, 606, 3229, 31, 1687, 1569, 8, 4564, 25, 5476, 7, 20, 569, 329, 3538, 8, 293, 420, 4196, 207, 332, 18, 1405, 5128, 101, 9, 3, 748, 2743, 14, 845, 618, 8, 2699, 25, 1910, 98, 3, 84, 1293, 4, 1951, 473, 53, 1570, 1207, 23, 4572, 2566, 2024, 31, 5, 1411, 154, 99, 232, 5716, 1269, 14, 1600, 156, 540, 899, 767, 2487, 671, 529, 7, 810, 1216, 4, 3220, 10, 2076, 791, 899, 767, 1487, 93, 737, 421, 239, 247, 29, 105, 6000, 3, 111, 430, 406, 3371, 767, 2203, 1738, 1191, 590, 265, 899, 767, 1172, 44, 80, 1435, 7, 1951, 473, 3257, 776, 46, 5324, 66, 376, 364, 18, 1021, 3, 2743, 606, 4, 1951, 473, 10, 1956, 25, 1103, 9, 148, 1650, 2096, 250, 2653, 2076, 407, 40, 89, 5896, 3, 59, 19, 4, 1560, 2943, 10, 580, 360, 2228, 73, 430, 406, 3371, 767, 2743, 606, 3008, 4, 16, 4331, 4, 16, 4566, 5604, 1004, 9, 236, 2244, 2796, 133, 23, 40, 767, 41, 29, 785, 54, 2954, 331, 40, 2778, 406, 841, 706, 267, 134, 97, 1595, 1991, 706, 18, 1021, 3439, 24, 1865, 198, 35, 5, 6, 6916, 1540, 4618, 6, 11, 1910, 98, 593, 1029, 4, 2100, 14, 1085, 5, 433, 24, 1910, 98, 24, 845, 618, 8, 2699, 5, 6, 6050, 2039, 1460, 11, 1172, 44, 80, 1435, 1078, 160, 120, 2523, 2920, 2919, 6051, 6, 11, 1105, 1285, 3, 84, 1293, 4, 1617, 2504, 232, 677, 21, 1324, 378, 8, 5642, 1165, 20, 4210, 3496, 4264, 278, 18, 6, 11, 6050, 740, 94, 6175, 93, 1773, 6051, 524, 4, 2722, 291, 9, 3764, 5134, 7, 2848, 41]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max step-length: 7313\n"
     ]
    }
   ],
   "source": [
    "#To check the maximum input steps among the entire dataset\n",
    "max = 0\n",
    "for elem in tokenized_and_encoded_inputs:\n",
    "    if len(elem) > max:\n",
    "        max = len(elem)\n",
    "        \n",
    "print('max step-length:', max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min step-length: 69\n"
     ]
    }
   ],
   "source": [
    "#To check the minimus input steps among the entire dataset\n",
    "min = 7313\n",
    "for elem in tokenized_and_encoded_inputs:\n",
    "    if len(elem) < min:\n",
    "        min = len(elem)\n",
    "        \n",
    "print('min step-length:', min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg step-length: 890.2934708836705\n"
     ]
    }
   ],
   "source": [
    "#To check the average input steps among the entire dataset\n",
    "sum = 0\n",
    "total_num = len(tokenized_and_encoded_inputs)\n",
    "for elem in tokenized_and_encoded_inputs:\n",
    "     sum += len(elem)\n",
    "        \n",
    "print('avg step-length:', sum/total_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 716"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def pad_or_truncate_inputs(data, max_len):\n",
    "    new_data = []\n",
    "    pad = 0\n",
    "        \n",
    "    for sample in tqdm(data):\n",
    "        if len(sample) >= max_len:\n",
    "            tmp = sample[:max_len]\n",
    "        else:\n",
    "            tmp = sample\n",
    "            num_of_pads_needed = max_len - len(sample)\n",
    "            for _ in range(num_of_pads_needed):\n",
    "                tmp.append(pad)\n",
    "                \n",
    "        new_data.append(tmp)\n",
    "        \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5893/5893 [00:00<00:00, 22383.93it/s]\n",
      "100%|██████████| 1474/1474 [00:00<00:00, 41136.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (5893, 716)\n",
      "x_test.shape: (1474, 716)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train = pad_or_truncate_inputs(x_train, max_len)\n",
    "x_test = pad_or_truncate_inputs(x_test, max_len)\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "\n",
    "print('x_train.shape:', x_train.shape)\n",
    "print('x_test.shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(x_train[0]): 716\n",
      "len(x_test[1]): 716\n",
      "len(x_test[2]): 716\n"
     ]
    }
   ],
   "source": [
    "print('len(x_train[0]):', len(x_train[0]))\n",
    "print('len(x_test[1]):', len(x_test[1]))\n",
    "print('len(x_test[2]):', len(x_test[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   6  522  268  268  334  163  317  317 2670  706  160  106  486 4491\n",
      "  497  192  531  106 5232  317 4349  751  918  317 4085 3281 1918  317\n",
      "   59  739 2383 2552  317 2300 2528   28   82  272  179  290   62  163\n",
      "  545  163  545 1915  290   65  545 1411 2203 1738 1191  590  265  899\n",
      "   18 1910   98    3   84 1293   97 1951  473   10    6  361   30    3\n",
      "   91 1570 1177    7  759  126  167   40 2858  345 3181 1779   32  357\n",
      " 2943   11  132   95  358  159 1227   99 5443 1910   98  593 1029   15\n",
      " 1172   44   80 1435  317  581   12    3   84 1293 6552  949   15  269\n",
      " 1866  317 1122   12   97 2743  606  361  478 1951  473    8 1825    3\n",
      "  860 1074   10 2139    8 4815   25 3257    8  789  530   17   16 4029\n",
      "    5 4306 1426   73   74 6140 6552   15  145  130 5026 4909  317  959\n",
      "   12    7 2172  207 1153    3  860 1074  261 2343 2758    9  198  789\n",
      "  342 2487  671  529 2627  861   15  994  838  131  317  581   12    3\n",
      " 2758   17 4398    4  122   84 4682 1987   15 7162   63 1941  317 1716\n",
      "   12   97 3083  711   61    5 2559    3   91    7 3989 1560 2943 2090\n",
      "    8 2692  727    3   52  569    7  284  877 1558  164  569  223    5\n",
      " 2743  606 3229   31 1687 1569    8 4564   25 5476    7   20  569  329\n",
      " 3538    8  293  420 4196  207  332   18 1405 5128  101    9    3  748\n",
      " 2743   14  845  618    8 2699   25 1910   98    3   84 1293    4 1951\n",
      "  473   53 1570 1207   23 4572 2566 2024   31    5 1411  154   99  232\n",
      " 5716 1269   14 1600  156  540  899  767 2487  671  529    7  810 1216\n",
      "    4 3220   10 2076  791  899  767 1487   93  737  421  239  247   29\n",
      "  105 6000    3  111  430  406 3371  767 2203 1738 1191  590  265  899\n",
      "  767 1172   44   80 1435    7 1951  473 3257  776   46 5324   66  376\n",
      "  364   18 1021    3 2743  606    4 1951  473   10 1956   25 1103    9\n",
      "  148 1650 2096  250 2653 2076  407   40   89 5896    3   59   19    4\n",
      " 1560 2943   10  580  360 2228   73  430  406 3371  767 2743  606 3008\n",
      "    4   16 4331    4   16 4566 5604 1004    9  236 2244 2796  133   23\n",
      "   40  767   41   29  785   54 2954  331   40 2778  406  841  706  267\n",
      "  134   97 1595 1991  706   18 1021 3439   24 1865  198   35    5    6\n",
      " 6916 1540 4618    6   11 1910   98  593 1029    4 2100   14 1085    5\n",
      "  433   24 1910   98   24  845  618    8 2699    5    6 6050 2039 1460\n",
      "   11 1172   44   80 1435 1078  160  120 2523 2920 2919 6051    6   11\n",
      " 1105 1285    3   84 1293    4 1617 2504  232  677   21 1324  378    8\n",
      " 5642 1165   20 4210 3496 4264  278   18    6   11 6050  740   94 6175\n",
      "   93 1773 6051  524    4 2722  291    9 3764 5134    7 2848   41    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train.shape: (5893, 9)\n",
      "y_test.shape: (1474, 9)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "y_train = to_categorical(y_train.astype('int32'), 9)\n",
    "y_test = to_categorical(y_test.astype('int32'), 9)\n",
    "\n",
    "print('y_train.shape:', y_train.shape)\n",
    "print('y_test.shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def save_data_as_tfrecord(X, Y, tfrecord_filename):\n",
    "    with tf.python_io.TFRecordWriter(tfrecord_filename) as w:\n",
    "        for x, y in tqdm(zip(X, Y)):\n",
    "            x = x.reshape(-1)\n",
    "            features = tf.train.Features(feature = {\n",
    "                'X': tf.train.Feature(float_list = tf.train.FloatList(value=x)),\n",
    "                'Y': tf.train.Feature(float_list = tf.train.FloatList(value=y))\n",
    "            })\n",
    "            \n",
    "            example = tf.train.Example(features=features)\n",
    "            w.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5893it [00:19, 303.51it/s]\n",
      "1474it [00:04, 304.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFRcord files are created for training and test data.\n",
      "CPU times: user 24.2 s, sys: 120 ms, total: 24.4 s\n",
      "Wall time: 24.3 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "save_data_as_tfrecord(x_train, y_train, 'train_with_sp.tfrecord')\n",
    "save_data_as_tfrecord(x_test, y_test, 'test_with_sp.tfrecord')\n",
    "print('TFRcord files are created for training and test data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
