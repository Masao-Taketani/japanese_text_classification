{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from random import shuffle\n",
    "import random\n",
    "\n",
    "def pre_process_data(filepath):\n",
    "    dataset = []\n",
    "    dirs = []\n",
    "    \n",
    "    for dir in os.listdir(filepath):\n",
    "        if os.path.isdir(filepath + dir):\n",
    "            dirs.append(filepath + dir)\n",
    "            \n",
    "    for i, dir_path in enumerate(dirs):\n",
    "        dir_name = dir_path.split('/')[-1]\n",
    "        label_id = i\n",
    "        print('label_id: {}, dir_name: {}'.format(label_id, dir_name))\n",
    "        \n",
    "        for filename in glob.glob(os.path.join(filepath, dir_name, dir_name + \"*.txt\")):\n",
    "            with open(filename, 'r' ,encoding=\"utf-8\") as f:\n",
    "                #datasets hold sets of tuples such as (label, input text)\n",
    "                dataset.append((label_id, f.read()))\n",
    "                \n",
    "    random.seed(1234)            \n",
    "    shuffle(dataset)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label_id: 0, dir_name: dokujo-tsushin\n",
      "label_id: 1, dir_name: it-life-hack\n",
      "label_id: 2, dir_name: kaden-channel\n",
      "label_id: 3, dir_name: livedoor-homme\n",
      "label_id: 4, dir_name: movie-enter\n",
      "label_id: 5, dir_name: peachy\n",
      "label_id: 6, dir_name: smax\n",
      "label_id: 7, dir_name: sports-watch\n",
      "label_id: 8, dir_name: topic-news\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7,\n",
       " 'http://news.livedoor.com/article/detail/6749212/\\n2012-07-12T08:10:00+0900\\n″ご立腹″の松木氏、関塚監督の采配について苦言呈す\\n11日、国立競技場で行われた、キリンチャレンジカップ2012・U-23日本代表×U-23ニュージーランド代表の一戦は、日本代表が1点を先制し、終始優勢に試合を進めながらも、後半ロスタイムにイージーミスからボールを奪われ、まさかの失点。試合は1-1のドローに終わった。\\n\\n12日、フジテレビで放送された「めざましテレビ」では、サッカー解説者の松木安太郎氏が昨夜の一戦を振り返った。「かなりご立腹」と紹介された松木氏。「まず若さが目立ちましたね。せっかくいいカタチでボールを取って、いいカタチで攻めるんですけど、最後のもう一工夫がなかったり、シュートが遅れたり。バランスがすごくよくなかった」と語る。\\n\\nすると、松木氏は、関塚監督の采配についても言及。バックアップ・メンバーである、山崎亮平（磐田）や米本拓司（FC東京）が起用されたことに触れると、「ちょっと気になったのが、昨日、バックアップ・メンバーがもともとの18名を使う前に出てきたんですね。彼らがいけないということではなくて、必勝態勢のゲームでありながら、本大会のシミュレーションもしたいということを考えると、まずはバックアップメンバーでない選手から使うのが普通じゃないかな。その辺が最後の失点というか、心の緩みみに繋がっていったような気がします」と苦言を呈した。\\n\\n・宮本恒靖氏、U-23日本代表に「リスクを冒さないとダメ」\\n・U-23日本代表、まさかのドロー。セルジオ氏は「まるで高校サッカーの決勝」\\n')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"../livedoor_data/text/\"\n",
    "\n",
    "dataset = pre_process_data(path)\n",
    "dataset[1234]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7367"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('日本国内', 0.6301000118255615),\n",
       " ('アメリカ', 0.571485698223114),\n",
       " ('韓国', 0.5515788793563843),\n",
       " ('日本の歴史', 0.5300577878952026),\n",
       " ('日本国', 0.5054277181625366),\n",
       " ('日本の経済史', 0.5011717081069946),\n",
       " ('日本の魚', 0.4976142942905426),\n",
       " ('米国', 0.4971674680709839),\n",
       " ('中国', 0.49715349078178406),\n",
       " ('アメリカ合衆国', 0.49622151255607605)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('model.vec', binary=False)\n",
    "model.most_similar(positive=['日本'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "\n",
    "def tokenize_and_vectorize(dataset):\n",
    "    vectorized_data = []\n",
    "    tokenizer = MeCab.Tagger(\"-Owakati -d /var/lib/mecab/dic/mecab-ipadic-neologd\")\n",
    "    for sample in dataset:\n",
    "        tokens_list = tokenizer.parse(sample[1]).split()\n",
    "        each_sample_input_vecs = []\n",
    "        for token in tokens_list:\n",
    "            try:\n",
    "                each_sample_input_vecs.append(model[token])\n",
    "            except KeyError:\n",
    "                pass\n",
    "            \n",
    "        vectorized_data.append(each_sample_input_vecs)\n",
    "        \n",
    "    return vectorized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_labels(dataset):\n",
    "    labels = []\n",
    "    for sample in dataset:\n",
    "        labels.append(sample[0])\n",
    "    \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_inputs = tokenize_and_vectorize(dataset)\n",
    "labels = collect_labels(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(vectorized_inputs): 7367\n",
      "len(vectorized_inputs[0]): 374\n",
      "len(vectorized_inputs[0][0]): 300\n",
      "len(labels): 7367\n"
     ]
    }
   ],
   "source": [
    "print('len(vectorized_inputs):', len(vectorized_inputs))\n",
    "print('len(vectorized_inputs[0]):', len(vectorized_inputs[0]))\n",
    "print('len(vectorized_inputs[0][0]):', len(vectorized_inputs[0][0]))\n",
    "print('len(labels):', len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_data = int(len(vectorized_inputs)* 0.8)\n",
    "\n",
    "x_train = vectorized_inputs[:split_data]\n",
    "x_test = vectorized_inputs[split_data:]\n",
    "y_train= labels[:split_data]\n",
    "y_test = labels[split_data:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max step-length: 5760\n"
     ]
    }
   ],
   "source": [
    "#To check the maximum input steps among the entire dataset\n",
    "max = 0\n",
    "for elem in vectorized_inputs:\n",
    "    if len(elem) > max:\n",
    "        max = len(elem)\n",
    "        \n",
    "print('max step-length:', max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min step-length: 31\n"
     ]
    }
   ],
   "source": [
    "#To check the minimus input steps among the entire dataset\n",
    "min = 5760\n",
    "for elem in vectorized_inputs:\n",
    "    if len(elem) < min:\n",
    "        min = len(elem)\n",
    "        \n",
    "print('min step-length:', min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg step-length: 635.5489344373558\n"
     ]
    }
   ],
   "source": [
    "#To check the average input steps among the entire dataset\n",
    "sum = 0\n",
    "total_num = len(vectorized_inputs)\n",
    "for elem in vectorized_inputs:\n",
    "     sum += len(elem)\n",
    "        \n",
    "print('avg step-length:', sum/total_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 512\n",
    "embedding_dims = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def pad_or_truncate_inputs(data, max_len):\n",
    "    new_data = []\n",
    "    pad_vec = []\n",
    "    for _ in range(len(data[0][0])):\n",
    "        pad_vec.append(0.0)\n",
    "        \n",
    "    print('len of pad_vec:', len(pad_vec))\n",
    "        \n",
    "    for sample in tqdm(data):\n",
    "        if len(sample) >= max_len:\n",
    "            tmp = sample[:max_len]\n",
    "        else:\n",
    "            tmp = sample\n",
    "            num_of_pad_vecs_needed = max_len - len(sample)\n",
    "            for _ in range(num_of_pad_vecs_needed):\n",
    "                tmp.append(pad_vec)\n",
    "                \n",
    "        new_data.append(tmp)\n",
    "        \n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/5893 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of pad_vec: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5893/5893 [00:00<00:00, 49708.16it/s]\n",
      "100%|██████████| 1474/1474 [00:00<00:00, 51514.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len of pad_vec: 300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape: (5893, 512, 300)\n",
      "x_test.shape: (1474, 512, 300)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train = pad_or_truncate_inputs(x_train, max_len)\n",
    "x_test = pad_or_truncate_inputs(x_test, max_len)\n",
    "x_train = np.reshape(x_train, (len(x_train), max_len, embedding_dims))\n",
    "x_test = np.reshape(x_test, (len(x_test), max_len, embedding_dims))\n",
    "\n",
    "print('x_train.shape:', x_train.shape)\n",
    "print('x_test.shape:', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train.shape: (5893, 9)\n",
      "y_test.shape: (1474, 9)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)\n",
    "y_train = to_categorical(y_train.astype('int32'), 9)\n",
    "y_test = to_categorical(y_test.astype('int32'), 9)\n",
    "\n",
    "print('y_train.shape:', y_train.shape)\n",
    "print('y_test.shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def save_data_as_tfrecord(X, Y, tfrecord_filename):\n",
    "    with tf.python_io.TFRecordWriter(tfrecord_filename) as w:\n",
    "        for x, y in tqdm(zip(X, Y)):\n",
    "            x = x.reshape(-1)\n",
    "            features = tf.train.Features(feature = {\n",
    "                'X': tf.train.Feature(float_list = tf.train.FloatList(value=x)),\n",
    "                'Y': tf.train.Feature(float_list = tf.train.FloatList(value=y))\n",
    "            })\n",
    "            \n",
    "            example = tf.train.Example(features=features)\n",
    "            w.write(example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5893it [51:22,  1.91it/s]\n",
      "1474it [12:49,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFRcord files are created for training and test data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "save_data_as_tfrecord(x_train, y_train, 'train.tfrecord')\n",
    "save_data_as_tfrecord(x_test, y_test, 'test.tfrecord')\n",
    "print('TFRcord files are created for training and test data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
